
#!/usr/bin/env python3
"""
æ¨¡æ‹Ÿå·¥å…·æ¨¡å— - ä¸ºæ•…éšœè¯Šæ–­æ™ºèƒ½ä½“æä¾›æ¨¡æ‹Ÿæ•°æ®
"""
from datetime import datetime
from typing import List, Dict, Any, Optional, Union, Tuple
import os
import re
import json
from typing import Dict, List, Any, Optional, Tuple
# å‡è®¾çš„ä»£ç ä»“åº“è·¯å¾„
CODE_BASE_PATH = "/mnt/codebase"  # ä½ å¯ä»¥ä¿®æ”¹ä¸ºå®é™…è·¯å¾„æˆ–ä½¿ç”¨ç¯å¢ƒå˜é‡


# æ’é™¤test_dataä»æ–‡ä»¶å±‚é¢å¯¼å…¥å¤±è´¥ï¼šä¿®æ”¹ä¸ºç»å¯¹å¯¼å…¥ï¼Œå»æ‰ç›¸å¯¹å¯¼å…¥çš„ç‚¹
try:
    # å°è¯•ä»å½“å‰ç›®å½•å¯¼å…¥
    from test_data import (
        generate_servers,
        generate_nginx_logs_for_server,
        generate_metrics_for_server
    )
except ImportError:
    # å¦‚æœå¤±è´¥ï¼Œå°è¯•ä»çˆ¶ç›®å½•å¯¼å…¥
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from test_data import (
        generate_servers,
        generate_nginx_logs_for_server,
        generate_metrics_for_server
    )

# å°è¯•å¯¼å…¥ MySQL mock æ•°æ®ç”Ÿæˆå™¨
try:
    from mysql_test_data import generate_mysql_logs_for_server
except ImportError:
    import sys, os
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from mysql_test_data import generate_mysql_logs_for_server

# å°è¯•å¯¼å…¥ Redis mock æ•°æ®ç”Ÿæˆå™¨
try:
    from redis_test_data import generate_redis_logs_for_server
except ImportError:
    import sys, os
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from redis_test_data import generate_redis_logs_for_server

# ==================== åŸå§‹å‡½æ•°ï¼ˆä¸è£…é¥°ï¼‰====================

def get_nginx_servers_raw() -> List[Dict[str, Any]]:
    """è·å–æ‰€æœ‰NginxæœåŠ¡å™¨çš„IPåœ°å€å’ŒåŸºæœ¬ä¿¡æ¯ã€‚"""
    print(f"[å·¥å…·è°ƒç”¨] get_nginx_servers() - è·å–æœåŠ¡å™¨åˆ—è¡¨")
    servers = generate_servers()
    print(f"  æ‰¾åˆ° {len(servers)} å°æœåŠ¡å™¨:")
    for server in servers:
        print(f"  - {server['ip']} ({server['role']}, åŒºåŸŸ: {server['region']})")
    return servers


def get_server_logs_simple_raw(
        server_ip: str,
        api_endpoint: str = None,
        keywords: Union[str, List[str]] = None
) -> List[Dict[str, Any]]:
    """
    è·å–æœåŠ¡å™¨æ—¥å¿—ï¼ˆNginxï¼‰ï¼Œå¹¶è¾“å‡ºç»Ÿä¸€æ—¥å¿—ç»“æ„ UnifiedLogV1ï¼š
    æ ¹æ®ã€å…³é”®è¯æˆ–è€…æ—¶é—´æˆ³ã€‘ã€åˆ†æ‰¹ã€‘æœç´¢å‡ºæ¥
    {
        "source": "nginx",
        "server_ip": "...",
        "timestamp": "...",
        "severity": "...",
        "operation": "GET /api/v2/data.json",
        "status": "502",
        "latency_ms": 200.5,
        "raw": "åŸå§‹æ—¥å¿—"
    }
    """
    print(f"[å·¥å…·è°ƒç”¨] get_server_logs_simple('{server_ip}', api_endpoint={api_endpoint}, keywords={keywords})")

    # ç”Ÿæˆæ—¥å¿—
    logs = generate_nginx_logs_for_server(server_ip, 60)

    # æŒ‰ æ¥å£è·¯å¾„ è¿‡æ»¤
    if api_endpoint:
        logs = [log for log in logs if api_endpoint in log]

    # æŒ‰å…³é”®è¯è¿‡æ»¤ï¼šä¸åŒºåˆ†å¤§å°å†™
    if keywords:
        if isinstance(keywords, str):
            keywords = [keywords]

        logs = [
            log for log in logs
            if any(k.lower() in log.lower() for k in keywords)
        ]

    print(f"[å·¥å…·è°ƒç”¨] æ‰¾åˆ° {len(logs)} æ¡ç›¸å…³æ—¥å¿—")

    # è§£æ Nginx æ—¥å¿— â†’ ç»Ÿä¸€ç»“æ„ UnifiedLogV1
    structured_logs = []

    for log in logs[:10]:  # ä»ç„¶åªå¤„ç†å‰10æ¡ï¼Œé¿å…LLMè´Ÿè½½è¿‡å¤§
        try:
            import re

            # è·¯å¾„
            path_match = re.search(r'"(GET|POST)\s+([^\s?]+)', log)
            method = path_match.group(1) if path_match else "UNKNOWN"
            path = path_match.group(2) if path_match else "unknown"

            # çŠ¶æ€ç 
            status_match = re.search(r'"\s+(\d{3})\s+', log)
            status_code = status_match.group(1) if status_match else "000"

            # å“åº”æ—¶é—´
            rt_match = re.search(r'([\d.]+)$', log)
            response_time = float(rt_match.group(1)) if rt_match else 0.0
            latency_ms = response_time * 1000

            # IP
            ip_match = re.match(r'(\S+)', log)
            client_ip = ip_match.group(1) if ip_match else "0.0.0.0"

            # æ—¶é—´æˆ³
            time_match = re.search(r'\[(.*?)\]', log)
            timestamp = time_match.group(1) if time_match else ""

            # ç»Ÿä¸€ç»“æ„ UnifiedLogV1
            structured_logs.append({
                "source": "nginx",
                "server_ip": server_ip,
                "timestamp": timestamp,
                "severity": "ERROR" if int(status_code) >= 500 else "INFO",
                "operation": f"{method} {path}",
                "status": status_code,
                "latency_ms": latency_ms,
                "raw": log
            })

        except Exception as e:
            print(f"[è­¦å‘Š] è§£ææ—¥å¿—å¤±è´¥: {e}")
            continue

    return structured_logs


def get_mysql_logs_simple_raw(
        server_ip: str,
        start_time: str = "",
        end_time: str = "",
        keywords: str = "",
        min_duration_s: float = 0.0,
        limit: int = 1000
) -> Tuple[List[Dict[str, Any]], Optional[str]]:
    """
    è·å– MySQL æ—¥å¿—ï¼ˆæ¨¡æ‹Ÿï¼‰ï¼Œå¹¶è§£æä¸ºç»Ÿä¸€æ—¥å¿—ç»“æ„ UnifiedLogV1 æ ¼å¼ã€‚
    """
    print(f"[å·¥å…·è°ƒç”¨] get_mysql_logs_simple - server_ip: {server_ip}")

    # å¤„ç† keywords å‚æ•°
    keywords_list = []
    if keywords:
        if isinstance(keywords, list):
            keywords = ",".join(str(k) for k in keywords)
            print(f"[è°ƒè¯•] è‡ªåŠ¨è½¬æ¢ keywords ä¸ºå­—ç¬¦ä¸²: {keywords}")
        keywords_list = [k.strip() for k in keywords.split(",") if k.strip()]

    # ç¡®ä¿å…¶ä»–å‚æ•°æœ‰åˆç†çš„é»˜è®¤å€¼
    start_time = start_time if start_time else None
    end_time = end_time if end_time else None
    min_duration_s_val = float(min_duration_s) if min_duration_s else 0.0

    print(
        f"  å‚æ•°: start_time={start_time}, end_time={end_time}, keywords={keywords_list}, min_duration_s={min_duration_s_val}, limit={limit}")

    # ä¿®å¤è¿™é‡Œï¼šå®‰å…¨åœ°å¤„ç† min_duration_s æ¯”è¾ƒ
    if min_duration_s is not None:
        min_duration_s_val = float(min_duration_s) if min_duration_s else 0.0
    else:
        min_duration_s_val = None

    # åŸæœ‰çš„ç”Ÿæˆå’Œè§£æé€»è¾‘ä¿æŒä¸å˜...
    # 1. ç”Ÿæˆæ—¥å¿—ï¼ˆåŸå§‹å­—ç¬¦ä¸²ï¼‰
    raw_logs = generate_mysql_logs_for_server(server_ip, 60)

    # è¾…åŠ©å‡½æ•°ï¼šè§£ææ—¶é—´å­—ç¬¦ä¸²ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
    def parse_time_string(time_str: str) -> Optional[datetime]:
        """è§£ææ—¶é—´å­—ç¬¦ä¸²ï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
        if not time_str:
            return None

        try:
            if 'T' in time_str:
                time_str = time_str.replace('T', ' ')
                if '.' in time_str:
                    time_str = time_str.split('.')[0]
        except Exception:
            pass

        try:
            return datetime.strptime(time_str, "%Y-%m-%d %H:%M:%S")
        except ValueError:
            try:
                return datetime.fromisoformat(time_str)
            except Exception:
                print(f"[è­¦å‘Š] æ— æ³•è§£ææ—¶é—´æ ¼å¼: {time_str}")
                return None

    # è¾…åŠ©å‡½æ•°ï¼šè§£ææ—¥å¿—æ—¶é—´æˆ³
    def parse_ts(log: str) -> Optional[datetime]:
        try:
            ts_str = log[:19]  # å‡è®¾æ ¼å¼ä¸º "YYYY-MM-DD HH:MM:SS"
            return datetime.strptime(ts_str, "%Y-%m-%d %H:%M:%S")
        except Exception:
            return None

    # å…³é”®è¯è¿‡æ»¤
    if keywords_list:
        raw_logs = [log for log in raw_logs if any(k.lower() in log.lower() for k in keywords_list)]

    # æœ€å°è€—æ—¶è¿‡æ»¤ï¼ˆç­›é€‰æ…¢ SQLï¼‰
    if min_duration_s_val and min_duration_s_val > 0:
        filtered = []
        for log in raw_logs:
            import re
            duration_match = re.search(r'duration=([\d.]+)s', log)
            if duration_match:
                duration = float(duration_match.group(1))
                if duration >= min_duration_s_val:
                    filtered.append(log)
        raw_logs = filtered

    # æ—¶é—´çª—è¿‡æ»¤ï¼ˆé™æµï¼‰
    if start_time:
        start_dt = parse_time_string(start_time)
        if start_dt:
            raw_logs = [log for log in raw_logs if parse_ts(log) and parse_ts(log) >= start_dt]

    if end_time:
        end_dt = parse_time_string(end_time)
        if end_dt:
            raw_logs = [log for log in raw_logs if parse_ts(log) and parse_ts(log) <= end_dt]

    # æ’åºï¼Œå°†æ‰€æœ‰æ—¥å¿—æŒ‰ç…§æ—¶é—´æˆ³å‡åºæ’åº
    raw_logs.sort(key=lambda x: parse_ts(x) or datetime.min)
    print(f"[å·¥å…·è°ƒç”¨] æ‰¾åˆ° {len(raw_logs)} æ¡ MySQL æ—¥å¿—")

    # è§£æ â†’ ç»Ÿä¸€ç»“æ„ UnifiedLogV1 â†’ å¹¶åªç­›é€‰limitæ¡æ—¥å¿—
    batch_logs = raw_logs[:limit]
    structured_logs = []
    next_start_time = None

    for log in batch_logs:
        try:
            import re
            # æ—¶é—´æˆ³
            ts = log[:19]
            # ä¸¥é‡çº§åˆ«
            sev_match = re.search(r"\[(INFO|WARN|ERROR)\]", log)
            severity = sev_match.group(1) if sev_match else "INFO"

            # SQL
            sql_match = re.search(r'sql="([^"]+)"', log)
            sql = sql_match.group(1) if sql_match else "UNKNOWN SQL"

            # è€—æ—¶
            dur_match = re.search(r'duration=([\d.]+)s', log)
            latency_ms = float(dur_match.group(1)) * 1000 if dur_match else 0.0

            status = "ERROR" if severity == "ERROR" else "OK"

            structured_logs.append({
                "source": "mysql",
                "server_ip": server_ip,
                "timestamp": ts,
                "severity": severity,
                "operation": sql,
                "status": status,
                "latency_ms": latency_ms,
                "raw": log
            })

            next_start_time = ts

        except Exception as e:
            print(f"[è­¦å‘Š] è§£æ MySQL æ—¥å¿—å¤±è´¥: {e}")
            continue

    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€é¡µ
    if len(batch_logs) < limit:
        next_start_time = None

    return structured_logs, next_start_time


def mysql_runtime_diagnosis_raw(
        server_ip: str,
        action: str,
) -> Dict[str, Any]:
    """
    MySQL è¿è¡Œæ—¶è¯Šæ–­å·¥å…·ï¼ˆæ¨¡æ‹Ÿï¼‰
    """
    print(f"[å·¥å…·è°ƒç”¨] mysql_runtime_diagnosis(server_ip={server_ip}, action={action})")

    if action == "processlist":
        return {
            "type": "processlist",
            "processes": [
                {
                    "id": 1234,
                    "user": "app_user",
                    "db": "order_db",
                    "time_sec": 85,
                    "state": "Waiting for lock",
                    "sql": "UPDATE orders SET status='PAID' WHERE id=10001"
                },
                {
                    "id": 1235,
                    "user": "report_user",
                    "db": "order_db",
                    "time_sec": 2,
                    "state": "Sending data",
                    "sql": "SELECT * FROM orders WHERE created_at > NOW() - INTERVAL 1 DAY"
                }
            ]
        }

    elif action == "innodb_status":
        return {
            "type": "innodb_status",
            "latest_deadlock": {
                "transaction_1": "UPDATE orders SET status='PAID' WHERE id=10001",
                "transaction_2": "UPDATE orders SET status='CANCEL' WHERE id=10001",
                "locked_table": "orders",
                "locked_index": "PRIMARY",
                "note": "ä¸¤ä¸ªäº‹åŠ¡äº’ç›¸ç­‰å¾…è¡Œé”ï¼Œäº§ç”Ÿæ­»é”"
            }
        }
    elif action == "variables":
        return {
            "type": "variables",
            "slow_query_log": "ON",
            "slow_query_log_file": "/var/log/mysql/slow.log",
            "long_query_time": 2,
            "max_connections": 500
        }

    elif action == "connections":
        return {
            "type": "connections",
            "threads_connected": 480,
            "threads_running": 120,
            "max_connections": 500,
            "warning": "è¿æ¥æ•°æ¥è¿‘ä¸Šé™"
        }

    else:
        return {
            "error": f"ä¸æ”¯æŒçš„è¯Šæ–­åŠ¨ä½œ: {action}"
        }


def get_redis_logs_simple_raw(
    server_ip: str,
    keywords: Optional[Union[str, List[str]]] = None,
    min_duration: Optional[float] = None,
    **kwargs
) -> List[Dict[str, Any]]:
    """
    è·å– Redis æ—¥å¿—å¹¶è§£ææˆ UnifiedLogV1 æ ¼å¼
    """
    print(f"[å·¥å…·è°ƒç”¨] get_redis_logs_simple('{server_ip}', keywords={keywords}, min_duration_s={min_duration})")

    logs = generate_redis_logs_for_server(server_ip, 60)

    # å…³é”®è¯è¿‡æ»¤
    if keywords:
        if isinstance(keywords, str):
            keywords = [keywords]
        logs = [log for log in logs if any(k.lower() in log.lower() for k in keywords)]

    # æœ€å°è€—æ—¶è¿‡æ»¤
    if min_duration:
        filtered = []
        for log in logs:
            import re
            dur_match = re.search(r'duration=(\d+)ms', log)
            if dur_match:
                dur_ms = int(dur_match.group(1))
                if dur_ms >= min_duration * 1000:
                    filtered.append(log)
        logs = filtered

    # ç»Ÿä¸€ç»“æ„åŒ–
    structured = []
    import re

    for log in logs[:15]:
        try:
            ts = re.match(r"(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})", log).group(1)
            severity = re.search(r"\[(INFO|WARN|ERROR|SLOWLOG)\]", log).group(1)

            cmd_match = re.search(r'command="([^"]+)"', log)
            command = cmd_match.group(1) if cmd_match else "UNKNOWN"

            dur_match = re.search(r'duration=(\d+)ms', log)
            latency_ms = int(dur_match.group(1)) if dur_match else 0

            status = "ERROR" if "ERROR" in severity else "OK"

            structured.append({
                "source": "redis",
                "server_ip": server_ip,
                "timestamp": ts,
                "severity": severity,
                "operation": command,
                "status": status,
                "latency_ms": latency_ms,
                "raw": log
            })

        except Exception as e:
            print(f"[è­¦å‘Š] Redis æ—¥å¿—è§£æå¤±è´¥: {e}")
            continue

    return structured


def get_server_metrics_simple_raw(
        server_ip: str,
        metric_name: Union[str, List[str]] = None
) -> Dict[str, Any]:
    """
    ç®€åŒ–çš„æŒ‡æ ‡è·å–å·¥å…·ï¼Œæ”¯æŒæ‰¹é‡æŸ¥è¯¢å’Œæ™ºèƒ½åç§°æ˜ å°„ã€‚
    """
    print(f"[å·¥å…·è°ƒç”¨] get_server_metrics_simple('{server_ip}', metric_name={metric_name})")

    # ç”Ÿæˆæ‰€æœ‰æ¨¡æ‹ŸæŒ‡æ ‡
    all_metrics = generate_metrics_for_server(server_ip, 60)

    # ç®€åŒ–çš„æŒ‡æ ‡åç§°æ˜ å°„è¡¨
    metric_mapping = {
        # CPUç›¸å…³
        "cpu": "cpu_percent",
        "cpu_usage": "cpu_percent",
        "cpu_percent": "cpu_percent",
        "cpu_load": "cpu_percent",

        # å†…å­˜ç›¸å…³
        "memory": "memory_percent",
        "memory_usage": "memory_percent",
        "memory_percent": "memory_percent",
        "ram_usage": "memory_percent",

        # æˆåŠŸç‡ç›¸å…³
        "success_rate": "success_rate",
        "request_success_rate": "success_rate",
        "success": "success_rate",

        # å»¶è¿Ÿç›¸å…³
        "latency": "avg_latency_ms",
        "avg_latency": "avg_latency_ms",
        "response_time": "avg_latency_ms",
        "avg_response_time": "avg_latency_ms",

        # è¿æ¥æ•°ç›¸å…³
        "active_connections": "active_connections",
        "connections": "active_connections",
        "connection_count": "active_connections",

        # å…¶ä»–å¸¸ç”¨åˆ«å
        "requests_per_sec": "requests_per_sec",
        "rps": "requests_per_sec",
        "qps": "requests_per_sec",
        "throughput": "requests_per_sec",

        "error_rate": "error_rate",
        "failure_rate": "error_rate",
    }

    # æ·»åŠ  error_rateï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if "error_rate" not in all_metrics and "success_rate" in all_metrics:
        all_metrics["error_rate"] = 100 - all_metrics["success_rate"]

    # 1. å¦‚æœ metric_name ä¸º Noneï¼Œè¿”å›æ‰€æœ‰æŒ‡æ ‡
    if metric_name is None:
        print(f"  æœªæŒ‡å®šæŒ‡æ ‡åç§°ï¼Œè¿”å›æ‰€æœ‰ {len(all_metrics)} ä¸ªæŒ‡æ ‡")
        return all_metrics

    # 2. å¤„ç†å­—ç¬¦ä¸²ç±»å‹çš„ metric_name
    elif isinstance(metric_name, str):
        # ç‰¹æ®Šå…³é”®å­— "all" ä»ç„¶æ”¯æŒ
        if metric_name.lower() == "all":
            print(f"  å…³é”®å­— 'all'ï¼Œè¿”å›æ‰€æœ‰ {len(all_metrics)} ä¸ªæŒ‡æ ‡")
            return all_metrics

        # å°è¯•æ˜ å°„æŒ‡æ ‡åç§°
        actual_key = metric_mapping.get(metric_name, metric_name)

        if actual_key in all_metrics:
            return {actual_key: all_metrics[actual_key]}
        else:
            # è¿”å›å¯ç”¨æŒ‡æ ‡åˆ—è¡¨å’Œå»ºè®®
            available_metrics = list(all_metrics.keys())
            return {
                "error": f"æŒ‡æ ‡ '{metric_name}' ä¸å­˜åœ¨",
                "available_metrics": available_metrics,
                "common_aliases": {
                    "cpu": ["cpu_usage", "cpu_percent"],
                    "memory": ["memory_usage", "memory_percent"],
                    "success_rate": ["request_success_rate"],
                    "latency": ["avg_latency_ms", "response_time"]
                }
            }

    # 3. å¤„ç†åˆ—è¡¨ç±»å‹çš„ metric_nameï¼ˆæ‰¹é‡æŸ¥è¯¢ï¼‰
    elif isinstance(metric_name, list):
        result = {}
        not_found = []

        for name in metric_name:
            if isinstance(name, str):
                # æ˜ å°„æŒ‡æ ‡åç§°
                mapped_key = metric_mapping.get(name, name)

                if mapped_key in all_metrics:
                    result[mapped_key] = all_metrics[mapped_key]
                else:
                    result[name] = "æŒ‡æ ‡ä¸å­˜åœ¨"
                    not_found.append(name)

        print(f"  æ‰¹é‡æŸ¥è¯¢ {len(metric_name)} ä¸ªæŒ‡æ ‡ï¼ŒæˆåŠŸè·å– {len(result) - len(not_found)} ä¸ª")

        response = {
            "server_ip": server_ip,
            "metrics": result,
            "total_requested": len(metric_name),
            "found": len(result) - len(not_found),
            "not_found": not_found if not_found else None,
            "timestamp": datetime.now().isoformat()
        }

        return response

    # 4. å…¶ä»–ç±»å‹
    else:
        return {
            "error": f"ä¸æ”¯æŒçš„ metric_name ç±»å‹: {type(metric_name)}",
            "supported_types": ["str", "list", "None"],
            "examples": {
                "è·å–æ‰€æœ‰æŒ‡æ ‡": {"metric_name": None},
                "è·å–å•ä¸ªæŒ‡æ ‡": {"metric_name": "cpu_percent"},
                "è·å–å¤šä¸ªæŒ‡æ ‡": {"metric_name": ["cpu_percent", "memory_percent", "success_rate"]}
            }
        }

def search_code_in_repository_raw(
        file_pattern: str = "*.py",
        keyword: str = None,
        file_path: str = None
) -> Dict[str, Any]:
    """
    åœ¨ä»£ç ä»“åº“ä¸­æœç´¢ç‰¹å®šæ–‡ä»¶æˆ–åŒ…å«å…³é”®å­—çš„ä»£ç 

    Args:
        file_pattern: æ–‡ä»¶æ¨¡å¼ï¼Œå¦‚ "*.py", "*.java"
        keyword: æœç´¢çš„å…³é”®å­—
        file_path: ç›´æ¥æŒ‡å®šæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœæœ‰ï¼‰

    Returns:
        æœç´¢ç»“æœçš„å­—å…¸
    """
    # æ–°å¢ï¼šå¤„ç†æ™ºèƒ½ä½“å¯èƒ½ä¼ å…¥çš„åˆ—è¡¨å‚æ•°
    import json
    import sys

    # å¦‚æœä¼ å…¥çš„æ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æä¸ºJSON
    if isinstance(file_pattern, str) and file_pattern.startswith('['):
        try:
            params_list = json.loads(file_pattern)
            # å–ç¬¬ä¸€ä¸ªå‚æ•°é›†åˆ
            if params_list and isinstance(params_list, list) and len(params_list) > 0:
                first_params = params_list[0]
                file_pattern = first_params.get('file_pattern', "*.py")
                keyword = first_params.get('keyword', keyword)
                file_path = first_params.get('file_path', file_path)
        except:
            pass  # å¦‚æœè§£æå¤±è´¥ï¼Œä¿æŒåŸæ ·

    print(
        f"[å·¥å…·è°ƒç”¨] search_code_in_repository(file_pattern={file_pattern}, keyword={keyword}, file_path={file_path})")

    # å¦‚æœæ˜¯ç›´æ¥æŒ‡å®šæ–‡ä»¶è·¯å¾„ï¼Œç›´æ¥è¿”å›è¯¥æ–‡ä»¶
    if file_path:
        if os.path.exists(file_path):
            return {
                "type": "direct_file",
                "file_path": file_path,
                "exists": True,
                "suggestions": [f"å·²å®šä½åˆ°æ–‡ä»¶: {file_path}"]
            }
        else:
            # å°è¯•åœ¨ä»£ç ä»“åº“ä¸­æŸ¥æ‰¾
            file_path = os.path.join(CODE_BASE_PATH, file_path.lstrip('/'))
            if os.path.exists(file_path):
                return {
                    "type": "direct_file",
                    "file_path": file_path,
                    "exists": True,
                    "suggestions": [f"å·²å®šä½åˆ°æ–‡ä»¶: {file_path}"]
                }

    # æ¨¡æ‹Ÿæœç´¢ç»“æœ - å®é™…é¡¹ç›®ä¸­åº”è¯¥éå†ç›®å½•
    results = []

    # æ ¹æ®æ¥å£è·¯å¾„çŒœæµ‹å¯èƒ½çš„ä»£ç æ–‡ä»¶
    if keyword and "/api/" in keyword:
        # ä»APIè·¯å¾„æ¨æ–­ä»£ç æ–‡ä»¶
        api_path = keyword
        # ä¾‹å¦‚: /api/v2/data.json -> controllers/data_controller.py, views/data_view.py ç­‰
        parts = api_path.strip('/').split('/')
        if len(parts) >= 2:
            endpoint = parts[-1].replace('.json', '').replace('.', '_')

            # ç”Ÿæˆå¯èƒ½çš„æ–‡ä»¶è·¯å¾„
            possible_files = [
                f"app/controllers/{endpoint}_controller.py",
                f"app/api/v{parts[1] if parts[1].startswith('v') and len(parts) > 1 else '1'}/{endpoint}.py",
                f"src/routes/{endpoint}_routes.py",
                f"api/views/{endpoint}_view.py",
                f"handlers/{endpoint}_handler.py"
            ]

            for file in possible_files:
                full_path = os.path.join(CODE_BASE_PATH, file)
                results.append({
                    "file_path": file,
                    "full_path": full_path,
                    "confidence": "high",
                    "reason": f"æ ¹æ®APIè·¯å¾„ {api_path} æ¨æ–­"
                })

    # æ ¹æ®å…³é”®å­—æœç´¢ï¼ˆæ¨¡æ‹Ÿï¼‰
    if keyword:
        # æ¨¡æ‹Ÿå¸¸è§é—®é¢˜çš„ä»£ç æ–‡ä»¶
        common_problem_files = {
            "timeout": [
                {"file": "app/services/order_service.py", "line": 45, "code": "time.sleep(5)"},
                {"file": "app/utils/network_utils.py", "line": 78, "code": "requests.get(url, timeout=None)"}
            ],
            "memory": [
                {"file": "app/utils/cache_manager.py", "line": 120, "code": "cache = []  # å†…å­˜æ³„æ¼é£é™©"},
                {"file": "app/services/data_service.py", "line": 33,
                 "code": "data_list = []\nwhile True:\n    data_list.append(get_data())"}
            ],
            "deadlock": [
                {"file": "app/services/payment_service.py", "line": 67,
                 "code": "with lock1:\n    with lock2:\n        # å¤„ç†æ”¯ä»˜"},
                {"file": "app/utils/db_manager.py", "line": 89,
                 "code": "session1.query(User).filter(User.id==1).with_for_update()"}
            ],
            "502": [
                {"file": "app/controllers/api_controller.py", "line": 112,
                 "code": "response = requests.get('http://downstream-service')"},
                {"file": "app/middlewares/error_handler.py", "line": 56,
                 "code": "if status_code >= 500:\n    return '502 Bad Gateway'"}
            ]
        }

        for problem_type, files in common_problem_files.items():
            if problem_type in keyword.lower():
                for file_info in files:
                    results.append({
                        "file_path": file_info["file"],
                        "full_path": os.path.join(CODE_BASE_PATH, file_info["file"]),
                        "confidence": "medium",
                        "reason": f"å¸¸è§{problem_type}é—®é¢˜ç›¸å…³æ–‡ä»¶",
                        "line": file_info["line"],
                        "sample_code": file_info["code"]
                    })

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å…·ä½“ç»“æœï¼Œè¿”å›é€šç”¨å»ºè®®
    if not results:
        results = [
            {
                "file_path": "app/controllers/",
                "full_path": os.path.join(CODE_BASE_PATH, "app/controllers"),
                "confidence": "low",
                "reason": "å»ºè®®æ£€æŸ¥æ§åˆ¶å™¨ç›®å½•"
            },
            {
                "file_path": "app/services/",
                "full_path": os.path.join(CODE_BASE_PATH, "app/services"),
                "confidence": "low",
                "reason": "å»ºè®®æ£€æŸ¥æœåŠ¡å±‚ä»£ç "
            }
        ]

    return {
        "search_results": results,
        "total_count": len(results),
        "keyword": keyword,
        "file_pattern": file_pattern
    }


def get_code_context_raw(
        file_path: str,
        line_start: int = 1,
        line_end: int = 50,
        highlight_lines: List[int] = None
) -> Dict[str, Any]:
    """
    è·å–ä»£ç æ–‡ä»¶çš„ä¸Šä¸‹æ–‡å†…å®¹

    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        line_start: èµ·å§‹è¡Œå·
        line_end: ç»“æŸè¡Œå·
        highlight_lines: éœ€è¦é«˜äº®æ˜¾ç¤ºçš„è¡Œå·åˆ—è¡¨

    Returns:
        ä»£ç å†…å®¹å’Œå…ƒæ•°æ®
    """
    print(f"[å·¥å…·è°ƒç”¨] get_code_context(file_path={file_path}, line_start={line_start}, line_end={line_end})")

    # å¤„ç†ç›¸å¯¹è·¯å¾„
    if not os.path.isabs(file_path):
        file_path = os.path.join(CODE_BASE_PATH, file_path.lstrip('/'))

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(file_path):
        return {
            "error": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}",
            "suggestions": [
                f"è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®",
                f"å¯ä»¥å°è¯•ä½¿ç”¨ search_code_in_repository å·¥å…·æœç´¢"
            ]
        }

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        total_lines = len(lines)

        # ç¡®ä¿è¡Œå·åœ¨æœ‰æ•ˆèŒƒå›´å†…
        line_start = max(1, min(line_start, total_lines))
        line_end = max(line_start, min(line_end, total_lines))

        # è·å–æŒ‡å®šè¡ŒèŒƒå›´å†…çš„ä»£ç 
        code_snippet = lines[line_start - 1:line_end]

        # æ„å»ºè¡Œå·æ˜ å°„
        code_with_lines = []
        for i, line in enumerate(code_snippet, start=line_start):
            is_highlighted = highlight_lines and i in highlight_lines
            code_with_lines.append({
                "line_number": i,
                "content": line.rstrip('\n'),
                "highlighted": is_highlighted
            })

        # åˆ†æä»£ç ç‰¹å¾ï¼ˆç®€å•ç‰ˆï¼‰
        issues = []
        for i, line_info in enumerate(code_with_lines):
            line = line_info["content"]

            # æ£€æŸ¥å¸¸è§é—®é¢˜æ¨¡å¼
            if "time.sleep(" in line and "time.sleep(0.1)" not in line:
                issues.append({
                    "line": line_info["line_number"],
                    "type": "æ€§èƒ½é—®é¢˜",
                    "description": "é•¿æ—¶é—´sleepå¯èƒ½å¯¼è‡´è¯·æ±‚è¶…æ—¶",
                    "severity": "high"
                })

            if "while True:" in line and "break" not in "".join([l["content"] for l in code_with_lines[i:i + 10]]):
                issues.append({
                    "line": line_info["line_number"],
                    "type": "æ— é™å¾ªç¯é£é™©",
                    "description": "å¯èƒ½ç¼ºå°‘å¾ªç¯ç»ˆæ­¢æ¡ä»¶",
                    "severity": "high"
                })

            if "requests.get(" in line and "timeout=" not in line:
                issues.append({
                    "line": line_info["line_number"],
                    "type": "ç½‘ç»œè¯·æ±‚è¶…æ—¶",
                    "description": "ç¼ºå°‘timeoutå‚æ•°å¯èƒ½å¯¼è‡´è¯·æ±‚æŒ‚èµ·",
                    "severity": "medium"
                })

            if "session.query(" in line and ".all()" in line:
                issues.append({
                    "line": line_info["line_number"],
                    "type": "æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–",
                    "description": "è€ƒè™‘ä½¿ç”¨åˆ†é¡µæŸ¥è¯¢é¿å…å†…å­˜æº¢å‡º",
                    "severity": "medium"
                })

        return {
            "file_path": file_path,
            "total_lines": total_lines,
            "line_start": line_start,
            "line_end": line_end,
            "code": code_with_lines,
            "issues_found": issues,
            "language": "python" if file_path.endswith('.py') else
            "java" if file_path.endswith('.java') else
            "javascript" if file_path.endswith('.js') else "unknown"
        }

    except Exception as e:
        return {
            "error": f"è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}",
            "file_path": file_path
        }


def analyze_code_pattern_raw(
        code_snippet: str,
        issue_type: str = None
) -> Dict[str, Any]:
    """
    åˆ†æä»£ç ç‰‡æ®µï¼Œè¯†åˆ«å¸¸è§é—®é¢˜æ¨¡å¼

    Args:
        code_snippet: ä»£ç ç‰‡æ®µ
        issue_type: æŒ‡å®šè¦åˆ†æçš„é—®é¢˜ç±»å‹ï¼ˆå¯é€‰ï¼‰

    Returns:
        åˆ†æç»“æœ
    """
    print(f"[å·¥å…·è°ƒç”¨] analyze_code_pattern(issue_type={issue_type})")

    # å¸¸è§é—®é¢˜æ¨¡å¼æ£€æµ‹
    patterns = {
        "memory_leak": [
            # ä½¿ç”¨éè´ªå©ªåŒ¹é… .*? é¿å…åŒ¹é…è¿‡å¤š
            (r"\.append\(.*?\)\s*# æ²¡æœ‰æ¸…ç†", "åˆ—è¡¨ä¸æ–­è¿½åŠ å¯èƒ½å¯¼è‡´å†…å­˜æ³„æ¼"),
            (r"global\s+\w+\s*=\s*\[\]", "å…¨å±€å˜é‡ç´¯ç§¯æ•°æ®"),
            (r"while True:\s*\n\s*\w+\.append", "å¾ªç¯ä¸­ä¸æ–­è¿½åŠ åˆ°åˆ—è¡¨"),
            # ä¿®æ­£ï¼šä½¿ç”¨ [^)]* åŒ¹é…æ‹¬å·å†…ä»»æ„éå³æ‹¬å·å­—ç¬¦
            (r"PIL\.Image\.new\([^)]*\)\s*# æ²¡æœ‰å…³é—­", "å›¾ç‰‡èµ„æºæœªé‡Šæ”¾"),
            # è¿˜å¯ä»¥æ·»åŠ æ›´å¤šå¸¸è§å†…å­˜æ³„æ¼æ¨¡å¼ï¼š
            (r"open\([^)]*\)\s*(#.*)?$", "æ–‡ä»¶æ‰“å¼€åæ²¡æœ‰å…³é—­"),
            (r"connection\s*=\s*.+\.connect\(\)", "æ•°æ®åº“è¿æ¥æ²¡æœ‰å…³é—­"),
            (r"self\.cache\s*=\s*{}\s*# æ— é™å¢é•¿", "ç¼“å­˜å­—å…¸æ— é™å¢é•¿"),
            (r"\.add\(.*?\)\s*# é›†åˆä¸æ–­æ·»åŠ ", "é›†åˆä¸æ–­æ·»åŠ å…ƒç´ "),
            (r"threading\.Thread\(target=.*\)", "çº¿ç¨‹æ²¡æœ‰æ­£ç¡®ç®¡ç†")
        ],
        "deadlock": [
            (r"with lock[12]:\s*\n\s*with lock[21]:", "åµŒå¥—é”å¯èƒ½å¯¼è‡´æ­»é”"),
            (r"lock\.acquire\(\)\s*\n.*lock\.acquire\(\)", "é‡å¤è·å–é”"),
            (r"threading\.Lock\(\)\s*# å¤šçº¿ç¨‹æ­»é”é£é™©", "å¤šçº¿ç¨‹åŒæ­¥é—®é¢˜")
        ],
        "timeout": [
            (r"time\.sleep\([5-9]\)", "é•¿æ—¶é—´sleep"),
            (r"requests\.\w+\([^)]*timeout=None", "ç½‘ç»œè¯·æ±‚æœªè®¾ç½®è¶…æ—¶"),
            (r"while True:\s*if.*break", "å¯èƒ½æ— æ³•é€€å‡ºçš„å¾ªç¯"),
            (r"socket\.settimeout\(None\)", "socketæœªè®¾ç½®è¶…æ—¶")
        ],
        "database": [
            (r"\.all\(\)\s*# æŸ¥è¯¢æ‰€æœ‰æ•°æ®", "æœªåˆ†é¡µçš„å…¨è¡¨æŸ¥è¯¢"),
            (r"N\+1\s+query", "N+1æŸ¥è¯¢é—®é¢˜"),
            (r"SELECT \*\s+FROM", "SELECT * æ€§èƒ½é—®é¢˜"),
            (r"for.*in.*:\s*\n\s*session\.add", "å¾ªç¯ä¸­é€ä¸ªæ’å…¥æ•°æ®")
        ],
        "security": [
            (r"eval\(", "ä½¿ç”¨evalæœ‰å®‰å…¨é£é™©"),
            (r"exec\(", "ä½¿ç”¨execæœ‰å®‰å…¨é£é™©"),
            (r"subprocess\.call\(.*shell=True", "shellå‘½ä»¤æ³¨å…¥é£é™©"),
            (r"password\s*=\s*['\"]\w+['\"]", "ç¡¬ç¼–ç å¯†ç ")
        ]
    }

    findings = []

    # å¦‚æœæ²¡æœ‰æŒ‡å®šé—®é¢˜ç±»å‹ï¼Œæ£€æŸ¥æ‰€æœ‰ç±»å‹
    issue_types_to_check = [issue_type] if issue_type else patterns.keys()

    for check_type in issue_types_to_check:
        if check_type in patterns:
            for pattern, description in patterns[check_type]:
                matches = re.finditer(pattern, code_snippet, re.MULTILINE)
                for match in matches:
                    # è·å–åŒ¹é…çš„è¡Œ
                    line_start = code_snippet[:match.start()].count('\n') + 1
                    line_content = match.group(0).strip()

                    findings.append({
                        "issue_type": check_type,
                        "line": line_start,
                        "pattern": pattern,
                        "description": description,
                        "matched_code": line_content,
                        "severity": "high" if check_type in ["memory_leak", "deadlock"] else "medium"
                    })

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šé—®é¢˜ï¼Œè¿›è¡Œé€šç”¨åˆ†æ
    if not findings:
        # æ£€æŸ¥ä»£ç å¤æ‚åº¦
        lines = code_snippet.split('\n')

        # è®¡ç®—ä¸€äº›åŸºæœ¬æŒ‡æ ‡
        metrics = {
            "line_count": len(lines),
            "function_count": len(re.findall(r"def \w+", code_snippet)),
            "class_count": len(re.findall(r"class \w+", code_snippet)),
            "import_count": len(re.findall(r"import |from ", code_snippet)),
            "comment_ratio": sum(1 for line in lines if line.strip().startswith('#')) / len(lines) if lines else 0
        }

        # ç®€å•å¤æ‚åº¦åˆ†æ
        if metrics["line_count"] > 100:
            findings.append({
                "issue_type": "complexity",
                "description": "ä»£ç æ–‡ä»¶è¿‡é•¿ï¼Œå»ºè®®æ‹†åˆ†",
                "severity": "low",
                "metrics": metrics
            })

    return {
        "analysis_type": issue_type or "general",
        "findings": findings,
        "total_issues_found": len(findings),
        "summary": "å‘ç°{}ä¸ªæ½œåœ¨é—®é¢˜".format(len(findings)) if findings else "æœªå‘ç°æ˜æ˜¾é—®é¢˜"
    }

# ==================== ä½¿ç”¨@toolè£…é¥°çš„ç‰ˆæœ¬ï¼ˆä¾›CrewAIä½¿ç”¨ï¼‰====================
from crewai.tools import tool

@tool("è·å–NginxæœåŠ¡å™¨åˆ—è¡¨")
def get_nginx_servers() -> List[Dict[str, Any]]:
    """è·å–æ‰€æœ‰NginxæœåŠ¡å™¨çš„IPåœ°å€å’ŒåŸºæœ¬ä¿¡æ¯ã€‚"""
    return get_nginx_servers_raw()


@tool("è·å–æœåŠ¡å™¨æ—¥å¿—")
def get_server_logs_simple(
        server_ip: str,
        api_endpoint: str = None,
        keywords: Union[str, List[str]] = None
) -> List[Dict[str, Any]]:
    """è·å–æœåŠ¡å™¨æ—¥å¿—ï¼ˆNginxï¼‰ï¼Œå¹¶è¾“å‡ºç»Ÿä¸€æ—¥å¿—ç»“æ„ UnifiedLogV1"""
    return get_server_logs_simple_raw(server_ip, api_endpoint, keywords)


@tool("è·å–MySQLæ—¥å¿—")
def get_mysql_logs_simple(
        server_ip: str,
        start_time: str = "",
        end_time: str = "",
        keywords: str = "",
        min_duration_s: float = 0.0,
        limit: int = 1000
) -> Tuple[List[Dict[str, Any]], Optional[str]]:
    """è·å– MySQL æ—¥å¿—ï¼ˆæ¨¡æ‹Ÿï¼‰ï¼Œå¹¶è§£æä¸ºç»Ÿä¸€æ—¥å¿—ç»“æ„ UnifiedLogV1 æ ¼å¼ã€‚"""
    return get_mysql_logs_simple_raw(server_ip, start_time, end_time, keywords, min_duration_s, limit)


@tool("MYSQLè¿è¡Œæ—¶è¯Šæ–­")
def mysql_runtime_diagnosis(
        server_ip: str,
        action: str,
) -> Dict[str, Any]:
    """MySQL è¿è¡Œæ—¶è¯Šæ–­å·¥å…·ï¼ˆæ¨¡æ‹Ÿï¼‰"""
    return mysql_runtime_diagnosis_raw(server_ip, action)


@tool("è·å–Redisæ—¥å¿—")
def get_redis_logs_simple(
    server_ip: str,
    keywords: Optional[Union[str, List[str]]] = None,
    min_duration: Optional[float] = None,
    **kwargs
) -> List[Dict[str, Any]]:
    """è·å– Redis æ—¥å¿—å¹¶è§£ææˆ UnifiedLogV1 æ ¼å¼"""
    return get_redis_logs_simple_raw(server_ip, keywords, min_duration, **kwargs)

@tool("è·å–æœåŠ¡å™¨æŒ‡æ ‡")
def get_server_metrics_simple(
        server_ip: str,
        metric_name: Union[str, List[str]] = None
) -> Dict[str, Any]:
    """ç®€åŒ–çš„æŒ‡æ ‡è·å–å·¥å…·ï¼Œæ”¯æŒæ‰¹é‡æŸ¥è¯¢å’Œæ™ºèƒ½åç§°æ˜ å°„ã€‚"""
    return get_server_metrics_simple_raw(server_ip, metric_name)

@tool("æœç´¢ä»£ç ä»“åº“")
def search_code_in_repository(
        file_pattern: str = "*.py",
        keyword: str = None,
        file_path: str = None
) -> Dict[str, Any]:
    """åœ¨ä»£ç ä»“åº“ä¸­æœç´¢ç‰¹å®šæ–‡ä»¶æˆ–åŒ…å«å…³é”®å­—çš„ä»£ç """
    return search_code_in_repository_raw(file_pattern, keyword, file_path)


@tool("è·å–ä»£ç ä¸Šä¸‹æ–‡")
def get_code_context(
        file_path: str,
        line_start: int = 1,
        line_end: int = 50,
        highlight_lines: List[int] = None
) -> Dict[str, Any]:
    """è·å–ä»£ç æ–‡ä»¶çš„ä¸Šä¸‹æ–‡å†…å®¹"""
    return get_code_context_raw(file_path, line_start, line_end, highlight_lines)


@tool("åˆ†æä»£ç æ¨¡å¼")
def analyze_code_pattern(
        code_snippet: str,
        issue_type: str = None
) -> Dict[str, Any]:
    """åˆ†æä»£ç ç‰‡æ®µï¼Œè¯†åˆ«å¸¸è§é—®é¢˜æ¨¡å¼"""
    return analyze_code_pattern_raw(code_snippet, issue_type)
# ==================== æµ‹è¯•å‡½æ•° ====================

def test_tools_locally():
    """æœ¬åœ°æµ‹è¯•å·¥å…·å‡½æ•°"""
    print("ğŸ”§ æœ¬åœ°æµ‹è¯•å·¥å…·å‡½æ•°")

    # æµ‹è¯•æœåŠ¡å™¨åˆ—è¡¨
    servers = get_nginx_servers_raw()
    print(f"è·å–åˆ° {len(servers)} å°æœåŠ¡å™¨")

    # æµ‹è¯•è·å–ç‰¹å®šæœåŠ¡å™¨çš„æ—¥å¿—
    test_server = "10.0.2.101"
    print(f"\næµ‹è¯•æœåŠ¡å™¨ {test_server} çš„æ—¥å¿—:")
    logs = get_server_logs_simple_raw(test_server, api_endpoint="/api/v2/data.json")
    print(f"è·å–åˆ° {len(logs)} æ¡æ—¥å¿—")

    if logs:
        for log in logs[:3]:
            print(f"  - ä¸¥é‡çº§åˆ«: {log['severity']}, æ“ä½œ: {log['operation']}")

    # æµ‹è¯•è·å–æŒ‡æ ‡
    print(f"\næµ‹è¯•æœåŠ¡å™¨ {test_server} çš„æŒ‡æ ‡:")
    metrics = get_server_metrics_simple_raw(test_server, metric_name="cpu")
    print(f"CPUä½¿ç”¨ç‡: {metrics.get('cpu_percent', 'N/A')}%")

    print("\nâœ… æœ¬åœ°æµ‹è¯•å®Œæˆ")


# ==================== ä½¿ç”¨@toolè£…é¥°çš„ç‰ˆæœ¬ï¼ˆä¾›CrewAIä½¿ç”¨ï¼‰====================

if __name__ == "__main__":
    test_tools_locally()
