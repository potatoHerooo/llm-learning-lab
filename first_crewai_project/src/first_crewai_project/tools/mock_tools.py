#!/usr/bin/env python3
"""
æ¨¡æ‹Ÿå·¥å…·æ¨¡å— - ä¸ºæ•…éšœè¯Šæ–­æ™ºèƒ½ä½“æä¾›æ¨¡æ‹Ÿæ•°æ®
"""
from crewai.tools import tool
from datetime import datetime
from typing import List, Dict, Any, Optional, Union,Tuple

# æ’é™¤test_dataä»æ–‡ä»¶å±‚é¢å¯¼å…¥å¤±è´¥ï¼šä¿®æ”¹ä¸ºç»å¯¹å¯¼å…¥ï¼Œå»æ‰ç›¸å¯¹å¯¼å…¥çš„ç‚¹
try:
    # å°è¯•ä»å½“å‰ç›®å½•å¯¼å…¥
    from test_data import (
        generate_servers,
        generate_nginx_logs_for_server,
        generate_metrics_for_server
    )
except ImportError:
    # å¦‚æœå¤±è´¥ï¼Œå°è¯•ä»çˆ¶ç›®å½•å¯¼å…¥
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from test_data import (
        generate_servers,
        generate_nginx_logs_for_server,
        generate_metrics_for_server
    )

# å°è¯•å¯¼å…¥ MySQL mock æ•°æ®ç”Ÿæˆå™¨
try:
    from mysql_test_data import generate_mysql_logs_for_server
except ImportError:
    import sys, os
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from mysql_test_data import generate_mysql_logs_for_server

# å°è¯•å¯¼å…¥ Redis mock æ•°æ®ç”Ÿæˆå™¨
try:
    from redis_test_data import generate_redis_logs_for_server
except ImportError:
    import sys, os
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from redis_test_data import generate_redis_logs_for_server

# ==================== ç®€åŒ–çš„å·¥å…·ç‰ˆæœ¬ï¼ˆè§£å†³CrewAIéªŒè¯é—®é¢˜ï¼‰====================

@tool("è·å–NginxæœåŠ¡å™¨åˆ—è¡¨")
def get_nginx_servers() -> List[Dict[str, Any]]:
    """è·å–æ‰€æœ‰NginxæœåŠ¡å™¨çš„IPåœ°å€å’ŒåŸºæœ¬ä¿¡æ¯ã€‚"""
    print(f"[å·¥å…·è°ƒç”¨] get_nginx_servers() - è·å–æœåŠ¡å™¨åˆ—è¡¨")
    servers = generate_servers()
    print(f"  æ‰¾åˆ° {len(servers)} å°æœåŠ¡å™¨:")
    for server in servers:
        print(f"  - {server['ip']} ({server['role']}, åŒºåŸŸ: {server['region']})")
    return servers


@tool("è·å–æœåŠ¡å™¨æ—¥å¿—")
def get_server_logs_simple(
        server_ip: str,
        api_endpoint: str = None,
        keywords: Union[str, List[str]] = None
) -> List[Dict[str, Any]]:
    """
    è·å–æœåŠ¡å™¨æ—¥å¿—ï¼ˆNginxï¼‰ï¼Œå¹¶è¾“å‡ºç»Ÿä¸€æ—¥å¿—ç»“æ„ UnifiedLogV1ï¼š
    æ ¹æ®ã€å…³é”®è¯æˆ–è€…æ—¶é—´æˆ³ã€‘ã€åˆ†æ‰¹ã€‘æœç´¢å‡ºæ¥
    {
        "source": "nginx",
        "server_ip": "...",
        "timestamp": "...",
        "severity": "...",
        "operation": "GET /api/v2/data.json",
        "status": "502",
        "latency_ms": 200.5,
        "raw": "åŸå§‹æ—¥å¿—"
    }
    """
    print(f"[å·¥å…·è°ƒç”¨] get_server_logs_simple('{server_ip}', api_endpoint={api_endpoint}, keywords={keywords})")

    # ç”Ÿæˆæ—¥å¿—
    logs = generate_nginx_logs_for_server(server_ip, 60)

    # ------------------------------
    # â‘  æŒ‰ æ¥å£è·¯å¾„ è¿‡æ»¤
    # ------------------------------
    if api_endpoint:
        logs = [log for log in logs if api_endpoint in log]

    # ------------------------------
    # â‘¡ æŒ‰å…³é”®è¯è¿‡æ»¤ï¼šä¸åŒºåˆ†å¤§å°å†™
    # ------------------------------
    if keywords:
        if isinstance(keywords, str):
            keywords = [keywords]

        logs = [
            log for log in logs
            if any(k.lower() in log.lower() for k in keywords)
        ]

    print(f"[å·¥å…·è°ƒç”¨] æ‰¾åˆ° {len(logs)} æ¡ç›¸å…³æ—¥å¿—")

    # ------------------------------
    # â‘¢ è§£æ Nginx æ—¥å¿— â†’ ç»Ÿä¸€ç»“æ„ UnifiedLogV1
    # ------------------------------
    structured_logs = []

    for log in logs[:10]:  # ä»ç„¶åªå¤„ç†å‰10æ¡ï¼Œé¿å…LLMè´Ÿè½½è¿‡å¤§
        try:
            import re

            # è·¯å¾„
            path_match = re.search(r'"(GET|POST)\s+([^\s?]+)', log)
            method = path_match.group(1) if path_match else "UNKNOWN"
            path = path_match.group(2) if path_match else "unknown"

            # çŠ¶æ€ç 
            status_match = re.search(r'"\s+(\d{3})\s+', log)
            status_code = status_match.group(1) if status_match else "000"

            # å“åº”æ—¶é—´
            rt_match = re.search(r'([\d.]+)$', log)
            response_time = float(rt_match.group(1)) if rt_match else 0.0
            latency_ms = response_time * 1000

            # IP
            ip_match = re.match(r'(\S+)', log)
            client_ip = ip_match.group(1) if ip_match else "0.0.0.0"

            # æ—¶é—´æˆ³
            time_match = re.search(r'\[(.*?)\]', log)
            timestamp = time_match.group(1) if time_match else ""

            # ------------------------------
            # ç»Ÿä¸€ç»“æ„ UnifiedLogV1
            # ------------------------------
            structured_logs.append({
                "source": "nginx",
                "server_ip": server_ip,
                "timestamp": timestamp,
                "severity": "ERROR" if int(status_code) >= 500 else "INFO",
                "operation": f"{method} {path}",
                "status": status_code,
                "latency_ms": latency_ms,
                "raw": log
            })

        except Exception as e:
            print(f"[è­¦å‘Š] è§£ææ—¥å¿—å¤±è´¥: {e}")
            continue

    return structured_logs


@tool("è·å–MySQLæ—¥å¿—")
def get_mysql_logs_simple(
        server_ip: str,
        start_time: Optional[str] = None,
        end_time: Optional[str] = None,
        keywords: Optional[Union[str, List[str]]] = None,
        min_duration_s: Optional[float] = None,
        limit: int = 1000,
        **kwargs
) -> Tuple[List[Dict[str,Any]], Optional[str]]:
    """
    è·å– MySQL æ—¥å¿—ï¼ˆæ¨¡æ‹Ÿï¼‰ï¼Œå¹¶è§£æä¸ºç»Ÿä¸€æ—¥å¿—ç»“æ„ UnifiedLogV1 æ ¼å¼ã€‚
    å‚æ•°ï¼š
    - server_ipï¼šæœåŠ¡å™¨IPåœ°å€
    - start_time / end_time: æ—¶é—´çª—
    - keywords: ç­›é€‰å…³é”®è¯
    - min_duration_sï¼šç­›é€‰è¿è¡Œæ—¶é—´è¶…è¿‡æœ€å°è€—æ—¶çš„æ…¢SQLï¼ˆå•ä½æ˜¯ç§’ï¼‰
    - limit: è¿”å›æ—¥å¿—æ€»æ¡æ•°

    è¿”å›ç»“æ„ï¼š
    - logs: å½“å‰æ‰¹æ¬¡æ—¥å¿—ï¼ˆæœ€å¤šlimitæ¡ï¼‰
    - next_start_time: ä¸‹ä¸€æ¬¡æ‹‰å–çš„èµ·å§‹æ—¶é—´ï¼ˆè‹¥ä¸ºNoneè¡¨ç¤ºå·²å–å®Œï¼‰

    {
        "source": "mysql",
        "server_ip": "...",
        "timestamp": "...",
        "severity": "INFO" | "WARN" | "ERROR",
        "operation": "SELECT * FROM users",
        "status": "OK" | "ERROR",
        "latency_ms": 1234,
        "raw": "åŸå§‹æ—¥å¿—"
    }
    """
    print(
        f"[å·¥å…·è°ƒç”¨] get_mysql_logs_simple("
        f"server_ip={server_ip}, "
        f"start_time={start_time}, end_time={end_time}, "
        f"keywords={keywords}, min_duration_s={min_duration_s}, limit={limit})"
    )

    # 1. ç”Ÿæˆæ—¥å¿—ï¼ˆåŸå§‹å­—ç¬¦ä¸²ï¼‰
    raw_logs = generate_mysql_logs_for_server(server_ip, 60)

    # ------------------------------
    # â‘¡å…³é”®è¯è¿‡æ»¤
    # ------------------------------
    if keywords:
        if isinstance(keywords, str):
            keywords = [keywords]

        raw_logs = [
            log for log in raw_logs
            if any(k.lower() in log.lower() for k in keywords)
        ]

    # ------------------------------
    # â‘¢æœ€å°è€—æ—¶è¿‡æ»¤ï¼ˆç­›é€‰æ…¢ SQLï¼‰
    # ------------------------------
    if min_duration_s:
        filtered = []
        for log in raw_logs:
            import re
            duration_match = re.search(r'duration=([\d.]+)s', log)
            if duration_match:
                duration = float(duration_match.group(1))
                if duration >= min_duration_s:
                    filtered.append(log)
        raw_logs = filtered

    # ------------------------------
    # â‘£æ—¶é—´çª—è¿‡æ»¤ï¼ˆé™æµï¼‰
    # ------------------------------
    def parse_ts(log: str) -> Optional[datetime]:
        try:
            return datetime.strptime(log[:19] , "%Y-%m-%d %H:%M:%S")
        except Exception:
            return None

    if start_time:
        start_dt = datetime.strptime(start_time , "%Y-%m-%d %H:%M:%S")
        raw_logs = [
            log for log in raw_logs
            if parse_ts(log) and parse_ts(log) >= start_dt
        ]

    if end_time:
        end_dt = datetime.strptime(end_time , "%Y-%m-%d %H:%M:%S")
        raw_logs = [
            log for log in raw_logs
            if parse_ts(log) and parse_ts(log) <= end_dt
        ]

    #æ’åºï¼Œå°†æ‰€æœ‰æ—¥å¿—æŒ‰ç…§æ—¶é—´æˆ³å‡åºæ’åº
    raw_logs.sort(key = lambda x:parse_ts(x) or datetime.min)
    print(f"[å·¥å…·è°ƒç”¨] æ‰¾åˆ° {len(raw_logs)} æ¡ MySQL æ—¥å¿—")

    # ------------------------------
    # â‘¤è§£æ â†’ ç»Ÿä¸€ç»“æ„ UnifiedLogV1
    #       â†’ å¹¶åªç­›é€‰limitæ¡æ—¥å¿—
    # ------------------------------
    #æ‰¹æ¬¡åˆ‡ç‰‡
    batch_logs = raw_logs[:limit]
    structured_logs = []
    next_start_time = None

    for log in batch_logs:
        try:
            import re
            #æ—¶é—´æˆ³
            ts = log[:19]
            # ä¸¥é‡çº§åˆ«
            sev_match = re.search(r"\[(INFO|WARN|ERROR)\]", log)
            severity = sev_match.group(1) if sev_match else "INFO"

            # SQL
            sql_match = re.search(r'sql="([^"]+)"', log)
            sql = sql_match.group(1) if sql_match else "UNKNOWN SQL"

            # è€—æ—¶
            dur_match = re.search(r'duration=([\d.]+)s', log)
            latency_ms = float(dur_match.group(1)) * 1000 if dur_match else 0.0

            status = "ERROR" if severity == "ERROR" else "OK"

            structured_logs.append({
                "source": "mysql",
                "server_ip": server_ip,
                "timestamp": ts,
                "severity": severity,
                "operation": sql,
                "status": status,
                "latency_ms": latency_ms,
                "raw": log
            })

            next_start_time = ts

        except Exception as e:
            print(f"[è­¦å‘Š] è§£æ MySQL æ—¥å¿—å¤±è´¥: {e}")
            continue


    #æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€é¡µ
    if len(batch_logs) < limit:
        next_start_time = None

    return structured_logs,next_start_time

@tool("è·å–Redisæ—¥å¿—")
def get_redis_logs_simple(
    server_ip: str,
    keywords: Optional[Union[str, List[str]]] = None,
    min_duration: Optional[float] = None,
    **kwargs
) -> List[Dict[str, Any]]:
    """
    è·å– Redis æ—¥å¿—å¹¶è§£ææˆ UnifiedLogV1 æ ¼å¼
    """

    print(f"[å·¥å…·è°ƒç”¨] get_redis_logs_simple('{server_ip}', keywords={keywords}, min_duration_s={min_duration})")

    logs = generate_redis_logs_for_server(server_ip, 60)

    # å…³é”®è¯è¿‡æ»¤
    if keywords:
        if isinstance(keywords, str):
            keywords = [keywords]
        logs = [log for log in logs if any(k.lower() in log.lower() for k in keywords)]

    # æœ€å°è€—æ—¶è¿‡æ»¤
    if min_duration:
        filtered = []
        for log in logs:
            import re
            dur_match = re.search(r'duration=(\d+)ms', log)
            if dur_match:
                dur_ms = int(dur_match.group(1))
                if dur_ms >= min_duration * 1000:
                    filtered.append(log)
        logs = filtered

    # ç»Ÿä¸€ç»“æ„åŒ–
    structured = []
    import re

    for log in logs[:15]:
        try:
            ts = re.match(r"(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})", log).group(1)
            severity = re.search(r"\[(INFO|WARN|ERROR|SLOWLOG)\]", log).group(1)

            cmd_match = re.search(r'command="([^"]+)"', log)
            command = cmd_match.group(1) if cmd_match else "UNKNOWN"

            dur_match = re.search(r'duration=(\d+)ms', log)
            latency_ms = int(dur_match.group(1)) if dur_match else 0

            status = "ERROR" if "ERROR" in severity else "OK"

            structured.append({
                "source": "redis",
                "server_ip": server_ip,
                "timestamp": ts,
                "severity": severity,
                "operation": command,
                "status": status,
                "latency_ms": latency_ms,
                "raw": log
            })

        except Exception as e:
            print(f"[è­¦å‘Š] Redis æ—¥å¿—è§£æå¤±è´¥: {e}")
            continue

    return structured

@tool("è·å–æœåŠ¡å™¨æŒ‡æ ‡")
def get_server_metrics_simple(
        server_ip: str,
        metric_name: str = None
) -> Dict[str, Any]:
    """
    ç®€åŒ–çš„æŒ‡æ ‡è·å–å·¥å…·ï¼Œé¿å…å¤æ‚çš„å‚æ•°éªŒè¯é—®é¢˜ã€‚

    å‚æ•°:
        server_ip (str): æœåŠ¡å™¨IPåœ°å€
        metric_name (str): æŒ‡æ ‡åç§°ï¼ˆå¯é€‰ï¼‰

    è¿”å›:
        æŒ‡æ ‡æ•°æ®
    """
    print(f"[å·¥å…·è°ƒç”¨] get_server_metrics_simple('{server_ip}', metric_name={metric_name})")

    # ç”Ÿæˆæ¨¡æ‹ŸæŒ‡æ ‡
    all_metrics = generate_metrics_for_server(server_ip, 60)

    # è¿‡æ»¤æŒ‡å®šçš„æŒ‡æ ‡
    if metric_name:
        metric_mapping = {
            "cpu": "cpu_percent",
            "cpu_usage_total": "cpu_percent",
            "å†…å­˜": "memory_percent",
            "memory": "memory_percent",
            "ç£ç›˜": "disk_percent",
            "disk": "disk_percent",
            "æˆåŠŸç‡": "success_rate",
            "é”™è¯¯ç‡": "success_rate",
            "å»¶è¿Ÿ": "avg_latency_ms",
            "å“åº”æ—¶é—´": "avg_latency_ms"
        }

        actual_key = metric_mapping.get(metric_name.lower(), metric_name)
        if actual_key in all_metrics:
            return {actual_key: all_metrics[actual_key]}
        else:
            return {"error": f"æœªæ‰¾åˆ°æŒ‡æ ‡: {metric_name}"}
    else:
        # è¿”å›æ‰€æœ‰æŒ‡æ ‡
        return all_metrics


# ==================== åŸæ¥çš„å®Œæ•´ç‰ˆæœ¬ï¼ˆä»…ä¾›å†…éƒ¨ä½¿ç”¨ï¼‰ ====================

def get_server_logs_full(
        server_ip: str,
        keywords: Optional[Union[str, List[str]]] = None,
        start_time: Optional[str] = None,
        end_time: Optional[str] = None,
        time_range_minutes: int = 60,
        max_logs: int = 10000,
        error_codes: Optional[List[str]] = None,
        min_response_time: Optional[float] = None
) -> List[Dict[str, Any]]:
    """
    å®Œæ•´çš„æ—¥å¿—è·å–å‡½æ•°ï¼ˆä½†ä¸ç”¨ä½œCrewAIå·¥å…·ï¼‰
    """
    # ... å®Œæ•´å®ç°ï¼ˆä½†ä¸ç”¨@toolè£…é¥°å™¨ï¼‰...
    pass


def get_server_metrics_full(
        server_ip: str,
        time_range_minutes: int = 60,
        metric_name: Optional[Union[str, List[str]]] = None
) -> Dict[str, Any]:
    """
    å®Œæ•´çš„æŒ‡æ ‡è·å–å‡½æ•°ï¼ˆä½†ä¸ç”¨ä½œCrewAIå·¥å…·ï¼‰
    """
    # ... å®Œæ•´å®ç°ï¼ˆä½†ä¸ç”¨@toolè£…é¥°å™¨ï¼‰...
    pass


# åœ¨ mock_tools.py æ–‡ä»¶çš„æœ€åæ·»åŠ ï¼š

def test_tools_locally():
    """æœ¬åœ°æµ‹è¯•å·¥å…·å‡½æ•°"""
    print("ğŸ”§ æœ¬åœ°æµ‹è¯•å·¥å…·å‡½æ•°")

    # æµ‹è¯•æœåŠ¡å™¨åˆ—è¡¨
    servers = get_nginx_servers.function()
    print(f"è·å–åˆ° {len(servers)} å°æœåŠ¡å™¨")

    # æµ‹è¯•è·å–ç‰¹å®šæœåŠ¡å™¨çš„æ—¥å¿—
    test_server = "10.0.2.101"
    print(f"\næµ‹è¯•æœåŠ¡å™¨ {test_server} çš„æ—¥å¿—:")
    logs = get_server_logs_simple.function(test_server, api_endpoint="/api/v2/data.json")
    print(f"è·å–åˆ° {len(logs)} æ¡æ—¥å¿—")

    if logs:
        for log in logs[:3]:
            print(f"  - ä¸¥é‡çº§åˆ«: {log['severity']}, æ“ä½œ: {log['operation']}")

    # æµ‹è¯•è·å–æŒ‡æ ‡
    print(f"\næµ‹è¯•æœåŠ¡å™¨ {test_server} çš„æŒ‡æ ‡:")
    metrics = get_server_metrics_simple.function(test_server, metric_name="cpu")
    print(f"CPUä½¿ç”¨ç‡: {metrics.get('cpu_percent', 'N/A')}%")

    print("\nâœ… æœ¬åœ°æµ‹è¯•å®Œæˆ")


def verify_log_format():
    """éªŒè¯æ—¥å¿—æ ¼å¼æ˜¯å¦æ­£ç¡®"""
    print("ğŸ” éªŒè¯æ—¥å¿—æ ¼å¼")
    print("=" * 60)

    from test_data import generate_nginx_logs_for_server

    # ç”Ÿæˆæµ‹è¯•æ—¥å¿—
    test_logs = generate_nginx_logs_for_server("10.0.2.101", 1)  # ç”Ÿæˆå°‘é‡æ—¥å¿—

    if not test_logs:
        print("âŒ æ²¡æœ‰ç”Ÿæˆæ—¥å¿—ï¼")
        return

    print(f"ç”Ÿæˆ {len(test_logs)} æ¡æ—¥å¿—")
    print("\nç¬¬ä¸€æ¡æ—¥å¿—:")
    print(f"  {test_logs[0]}")

    # æ‰‹åŠ¨è§£æ
    log = test_logs[0]
    parts = log.split()

    print(f"\nåˆ†å‰²åå¾—åˆ° {len(parts)} éƒ¨åˆ†:")
    for i, part in enumerate(parts):
        print(f"  [{i}] {part}")

    print("\nå°è¯•è§£æ:")
    try:
        # æ–¹æ³•1ï¼šæŒ‰ç©ºæ ¼åˆ†å‰²
        ip = parts[0]
        timestamp = parts[3] + " " + parts[4]  # [01/Jan/2024:12:00:00 +0000]
        request = parts[5] + " " + parts[6] + " " + parts[7]  # "GET /api/v2/data.json HTTP/1.1"
        status_code = parts[8]
        response_size = parts[9]

        print(f"  IP: {ip}")
        print(f"  æ—¶é—´: {timestamp}")
        print(f"  è¯·æ±‚: {request}")
        print(f"  çŠ¶æ€ç : {status_code}")
        print(f"  å“åº”å¤§å°: {response_size}")

        # å‰©ä¸‹çš„éƒ¨åˆ†
        for i in range(10, len(parts)):
            print(f"  [{i}] {parts[i]}")

    except Exception as e:
        print(f"âŒ è§£æå¤±è´¥: {e}")


# åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ 
if __name__ == "__main__":
    verify_log_format()