# mock_problem_files_agent_focused.py
"""
æ ¹æ®æ™ºèƒ½ä½“å®é™…æœç´¢è·¯å¾„åˆ›å»ºæ¨¡æ‹Ÿæ–‡ä»¶
æ™ºèƒ½ä½“å®é™…åœ¨å¯»æ‰¾ï¼š
1. app/controllers/data_controller.py
2. app/api/v2/data.py
3. app/routes.py æˆ–ç±»ä¼¼è·¯ç”±æ–‡ä»¶
"""

import os
from pathlib import Path

CODE_BASE_PATH = "./mock_codebase"  # ä¿®æ”¹ä¸ºä½ çš„é¡¹ç›®è·¯å¾„


def create_agent_focused_files():
    """åˆ›å»ºæ™ºèƒ½ä½“å®é™…åœ¨å¯»æ‰¾çš„æ–‡ä»¶"""

    # æ™ºèƒ½ä½“ç¬¬ä¸€ä¸ªæƒ³çœ‹çš„æ–‡ä»¶
    controller_file = os.path.join(CODE_BASE_PATH, "app/controllers/data_controller.py")
    os.makedirs(os.path.dirname(controller_file), exist_ok=True)

    with open(controller_file, 'w') as f:
        f.write('''"""
æ•°æ®æ§åˆ¶å™¨ - å¤„ç† /api/v2/data.json ç›¸å…³è¯·æ±‚
æ™ºèƒ½ä½“ç¬¬ä¸€ä¼˜å…ˆçº§æ–‡ä»¶
åŒ…å«502é”™è¯¯ã€æ…¢æŸ¥è¯¢å’Œæ­»é”é—®é¢˜
"""

from flask import jsonify, request, abort
import time
import requests
from app.services.data_service import DataService
from app.utils.redis_client import RedisClient
from app.utils.db_manager import DatabaseManager

class DataController:
    def __init__(self):
        self.data_service = DataService()
        self.redis_client = RedisClient()
        self.db_manager = DatabaseManager()

        # å…¨å±€ç¼“å­˜ï¼ˆå¯èƒ½å†…å­˜æ³„æ¼ï¼‰
        self.request_cache = []  # é—®é¢˜ï¼šä»ä¸æ¸…ç†çš„ç¼“å­˜

    def get_data_v2(self):
        """
        å¤„ç† GET /api/v2/data.json è¯·æ±‚
        è¿™æ˜¯å¯¼è‡´502é”™è¯¯çš„ä¸»è¦å‡½æ•°
        """
        try:
            # é—®é¢˜1: è°ƒç”¨ä¸‹æ¸¸æœåŠ¡æ²¡æœ‰è¶…æ—¶è®¾ç½®
            downstream_url = "http://internal-data-service/api/raw"
            print(f"è°ƒç”¨ä¸‹æ¸¸æœåŠ¡: {downstream_url}")

            # å±é™©ï¼šç¼ºå°‘timeoutå‚æ•°ï¼Œå¯èƒ½å¯¼è‡´æ°¸ä¹…é˜»å¡
            response = requests.get(downstream_url)  # æ²¡æœ‰è®¾ç½®timeout

            # é—®é¢˜2: é”™è¯¯å¤„ç†ä¸å®Œæ•´
            if response.status_code != 200:
                # æ²¡æœ‰é‡è¯•æœºåˆ¶ï¼Œç›´æ¥è¿”å›é”™è¯¯
                return jsonify({"error": "ä¸‹æ¸¸æœåŠ¡å¼‚å¸¸", "code": 502}), 502

            data = response.json()

            # é—®é¢˜3: åŒæ­¥é˜»å¡å¤„ç†å¤§æ•°æ®
            processed_data = self._process_data_slowly(data)

            # é—®é¢˜4: æ²¡æœ‰ç¼“å­˜é™çº§
            cache_key = "data_v2_cache"
            self.redis_client.set(cache_key, processed_data, expire=300)

            # é—®é¢˜5: å‘å…¨å±€ç¼“å­˜æ·»åŠ æ•°æ®ï¼ˆå†…å­˜æ³„æ¼ï¼‰
            self.request_cache.append({
                "timestamp": time.time(),
                "data_size": len(str(processed_data))
            })

            return jsonify(processed_data)

        except requests.exceptions.ConnectionError as e:
            print(f"è¿æ¥é”™è¯¯: {e}")
            return jsonify({"error": "æ— æ³•è¿æ¥ä¸‹æ¸¸æœåŠ¡", "code": 502}), 502
        except Exception as e:
            print(f"æœªçŸ¥é”™è¯¯: {e}")
            return jsonify({"error": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯", "code": 500}), 500

    def _process_data_slowly(self, data):
        """
        ç¼“æ…¢å¤„ç†æ•°æ®ï¼ˆæ€§èƒ½ç“¶é¢ˆï¼‰
        åŒ…å«å¤šä¸ªæ€§èƒ½é—®é¢˜
        """
        import json

        # é—®é¢˜ï¼šåŒæ­¥JSONåºåˆ—åŒ–å¤§å¯¹è±¡
        json_str = json.dumps(data)  # å¦‚æœdataå¾ˆå¤§ï¼Œè¿™é‡Œä¼šé˜»å¡

        # é—®é¢˜ï¼šCPUå¯†é›†å‹æ“ä½œæ²¡æœ‰ä¼˜åŒ–
        result = []
        for i in range(len(data.get("items", []))):
            item = data["items"][i]

            # åµŒå¥—å¾ªç¯ï¼ˆO(n^2)å¤æ‚åº¦ï¼‰
            for j in range(10):
                # æ¨¡æ‹Ÿå¤æ‚è®¡ç®—
                processed = self._heavy_computation(item, j)
                result.append(processed)

            # é—®é¢˜ï¼šæ¯æ¬¡å¾ªç¯éƒ½æŸ¥è¯¢æ•°æ®åº“
            db_result = self.db_manager.query_item(item.get("id"))
            if db_result:
                result[-1]["db_info"] = db_result

        return {"items": result, "count": len(result)}

    def _heavy_computation(self, item, index):
        """CPUå¯†é›†å‹è®¡ç®—"""
        import hashlib
        import random

        # æ¨¡æ‹Ÿè€—æ—¶è®¡ç®—
        for _ in range(1000):
            hash_obj = hashlib.md5(str(item).encode())
            hash_obj.hexdigest()

        # éšæœºä¼‘çœ ï¼ˆå¢åŠ å»¶è¿Ÿï¼‰
        time.sleep(random.uniform(0.01, 0.05))

        return {"id": item.get("id"), "hash": hash_obj.hexdigest(), "index": index}

    def update_data(self, data_id):
        """
        æ›´æ–°æ•°æ®ï¼ˆå¯èƒ½å¯¼è‡´æ­»é”ï¼‰
        """
        from threading import Lock

        # é—®é¢˜ï¼šå…¨å±€é”å¯èƒ½å¯¼è‡´æ­»é”
        global_lock = Lock()
        db_lock = Lock()

        with global_lock:
            # è·å–æ•°æ®åº“è¿æ¥
            with db_lock:
                # æŸ¥è¯¢å½“å‰æ•°æ®
                current = self.db_manager.query(f"SELECT * FROM data WHERE id={data_id} FOR UPDATE")
                time.sleep(0.5)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´

                # æ›´æ–°æ“ä½œ
                self.db_manager.execute(f"UPDATE data SET updated_at=NOW() WHERE id={data_id}")

                # åŒæ—¶æ›´æ–°ç¼“å­˜ï¼ˆå¯èƒ½äº§ç”Ÿç«æ€æ¡ä»¶ï¼‰
                cache_key = f"data_{data_id}"
                self.redis_client.delete(cache_key)
                self.redis_client.set(cache_key, {"updated": True})

        return {"status": "success", "id": data_id}

    def batch_process(self, item_ids):
        """
        æ‰¹é‡å¤„ç†ï¼ˆN+1æŸ¥è¯¢é—®é¢˜ï¼‰
        """
        results = []

        # é—®é¢˜ï¼šå¾ªç¯ä¸­æŸ¥è¯¢æ•°æ®åº“ï¼ˆN+1é—®é¢˜ï¼‰
        for item_id in item_ids:
            # æ¯æ¬¡å¾ªç¯éƒ½æŸ¥è¯¢æ•°æ®åº“
            item_data = self.db_manager.query(f"SELECT * FROM data WHERE id={item_id}")

            # å†æ¬¡æŸ¥è¯¢å…³è”æ•°æ®
            related = self.db_manager.query(
                f"SELECT * FROM related_data WHERE data_id={item_id}"
            )

            # å†æ¬¡æŸ¥è¯¢ç»Ÿè®¡ä¿¡æ¯
            stats = self.db_manager.query(
                f"SELECT COUNT(*) as count FROM stats WHERE item_id={item_id}"
            )

            results.append({
                "item": item_data,
                "related": related,
                "stats": stats
            })

        # æ­£ç¡®åšæ³•åº”è¯¥æ˜¯ï¼šä½¿ç”¨JOINä¸€æ¬¡æ€§æŸ¥è¯¢æ‰€æœ‰æ•°æ®
        return results

# å…¨å±€å˜é‡ï¼ˆå†…å­˜æ³„æ¼é£é™©ï¼‰
GLOBAL_DATA_BUFFER = []

def add_to_global_buffer(data):
    """å‘å…¨å±€ç¼“å†²åŒºæ·»åŠ æ•°æ®ï¼Œä»ä¸æ¸…ç†"""
    GLOBAL_DATA_BUFFER.append(data)

    # è®°å½•æ—¥å¿—ï¼ˆå¯èƒ½äº§ç”Ÿå¤§é‡æ—¥å¿—ï¼‰
    import logging
    logging.basicConfig(level=logging.INFO)
    logging.info(f"Added data to buffer, size: {len(GLOBAL_DATA_BUFFER)}")

    return len(GLOBAL_DATA_BUFFER)
''')

    # æ™ºèƒ½ä½“ç¬¬äºŒä¸ªæƒ³çœ‹çš„æ–‡ä»¶ï¼ˆæ³¨æ„ï¼šæ™ºèƒ½ä½“å†™çš„æ˜¯ vv2ï¼Œåº”è¯¥æ˜¯ v2ï¼‰
    api_v2_file = os.path.join(CODE_BASE_PATH, "app/api/v2/data.py")
    os.makedirs(os.path.dirname(api_v2_file), exist_ok=True)

    with open(api_v2_file, 'w') as f:
        f.write('''"""
API v2 æ•°æ®ç«¯ç‚¹
æ™ºèƒ½ä½“ç¬¬äºŒä¼˜å…ˆçº§æ–‡ä»¶
åŒ…å«Redisç¼“å­˜é—®é¢˜å’Œè¿æ¥æ± æ³„æ¼
"""

from flask import Blueprint, jsonify, request, current_app
import time
import json
import threading
from app.utils.redis_client import RedisClient

data_bp = Blueprint('data_v2', __name__, url_prefix='/api/v2')

@data_bp.route('/data', methods=['GET'])
def get_data():
    """
    GET /api/v2/data
    ä¸»è¦é—®é¢˜ï¼šRedisç¼“å­˜ä½¿ç”¨ä¸å½“
    """
    # è·å–æŸ¥è¯¢å‚æ•°
    query = request.args.get('q', '')
    page = int(request.args.get('page', 1))
    size = int(request.args.get('size', 100))

    # é—®é¢˜1: ç¼“å­˜é”®è®¾è®¡ä¸å½“ï¼ˆå¯èƒ½å¯¼è‡´å¤§é‡ä¸åŒçš„ç¼“å­˜é”®ï¼‰
    cache_key = f"data_v2:{query}:{page}:{size}"

    redis_client = RedisClient()

    # é—®é¢˜2: ç¼“å­˜ç©¿é€ - æŸ¥è¯¢ä¸å­˜åœ¨çš„æ•°æ®
    try:
        cached_data = redis_client.get(cache_key)
        if cached_data:
            return jsonify(json.loads(cached_data))
    except Exception as e:
        current_app.logger.error(f"Redisè·å–å¤±è´¥: {e}")
        # æ²¡æœ‰é™çº§ç­–ç•¥ï¼Œç»§ç»­æ‰§è¡Œ

    # æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢ï¼ˆæ…¢æŸ¥è¯¢ï¼‰
    time.sleep(2.0)  # è¶…è¿‡æ…¢æŸ¥è¯¢é˜ˆå€¼

    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    data = {
        "query": query,
        "page": page,
        "size": size,
        "items": [{"id": i, "value": f"item_{i}"} for i in range(size)],
        "total": 1000
    }

    # é—®é¢˜3: ç¼“å­˜å¤§å¯¹è±¡ï¼ˆå¯èƒ½è¶…è¿‡Rediså†…å­˜é™åˆ¶ï¼‰
    # åºåˆ—åŒ–æ•´ä¸ªå¤§å¯¹è±¡
    data_json = json.dumps(data)

    # é—®é¢˜4: ç¼“å­˜æ²¡æœ‰è®¾ç½®è¿‡æœŸæ—¶é—´ï¼ˆæœ‰æ—¶è®¾ç½®äº†300ç§’ï¼Œæœ‰æ—¶æ°¸ä¹…ï¼‰
    if page == 1:
        redis_client.set(cache_key, data_json, expire=300)  # 5åˆ†é’Ÿ
    else:
        redis_client.set(cache_key, data_json)  # æ°¸ä¹…ç¼“å­˜ï¼Œå±é™©ï¼

    # é—®é¢˜5: è¿æ¥æ²¡æœ‰é‡Šæ”¾
    # redis_client.close()  # ç¼ºå°‘è¿™è¡Œä»£ç 

    return jsonify(data)

@data_bp.route('/data/<int:data_id>', methods=['GET'])
def get_data_by_id(data_id):
    """
    è·å–å•ä¸ªæ•°æ®
    é—®é¢˜ï¼šçƒ­ç‚¹æ•°æ®æ²¡æœ‰ç‰¹æ®Šå¤„ç†
    """
    cache_key = f"data_item:{data_id}"

    redis_client = RedisClient()

    # é—®é¢˜ï¼šç¼“å­˜å‡»ç©¿ - çƒ­ç‚¹æ•°æ®è¿‡æœŸæ—¶å¤§é‡è¯·æ±‚ç›´è¾¾æ•°æ®åº“
    try:
        cached = redis_client.get(cache_key)
        if cached:
            return jsonify(json.loads(cached))
    except:
        pass

    # æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢
    time.sleep(0.5)

    data = {
        "id": data_id,
        "name": f"Data Item {data_id}",
        "value": "x" * 1024,  # å¤§value
        "timestamp": time.time()
    }

    # é—®é¢˜ï¼šçƒ­ç‚¹æ•°æ®è®¾ç½®ç›¸åŒçš„è¿‡æœŸæ—¶é—´ï¼ˆå¯èƒ½å¯¼è‡´ç¼“å­˜é›ªå´©ï¼‰
    import random
    expire_time = 300 + random.randint(-30, 30)  # åº”è¯¥ä½¿ç”¨éšæœºè¿‡æœŸæ—¶é—´
    redis_client.set(cache_key, json.dumps(data), expire=expire_time)

    return jsonify(data)

@data_bp.route('/data', methods=['POST'])
def create_data():
    """
    åˆ›å»ºæ•°æ®
    é—®é¢˜ï¼šæ•°æ®åº“äº‹åŠ¡å’ŒRedisä¸ä¸€è‡´
    """
    data = request.json

    # é—®é¢˜1: å…ˆæ›´æ–°Redisï¼Œåæ›´æ–°æ•°æ®åº“ï¼ˆä¸ä¸€è‡´é£é™©ï¼‰
    redis_client = RedisClient()

    # ç”ŸæˆID
    import uuid
    data_id = str(uuid.uuid4())
    data['id'] = data_id

    # å…ˆç¼“å­˜
    cache_key = f"data_item:{data_id}"
    redis_client.set(cache_key, json.dumps(data), expire=3600)

    # ç„¶åæ•°æ®åº“ï¼ˆå¯èƒ½å¤±è´¥ï¼‰
    try:
        # æ¨¡æ‹Ÿæ•°æ®åº“æ“ä½œ
        time.sleep(1.5)

        # è¿™é‡Œå¯èƒ½å¤±è´¥ï¼Œä½†Rediså·²ç»æ›´æ–°äº†
        if "error" in data:
            raise Exception("æ¨¡æ‹Ÿæ•°æ®åº“é”™è¯¯")

        # é—®é¢˜2: æ²¡æœ‰æ¸…ç†ç›¸å…³ç¼“å­˜
        # åº”è¯¥æ¸…ç†åˆ—è¡¨ç¼“å­˜ï¼Œä½†æ²¡æœ‰åš
        # redis_client.delete("data_v2:*")

        return jsonify({"status": "success", "id": data_id}), 201

    except Exception as e:
        # æ•°æ®åº“å¤±è´¥ï¼Œä½†Rediså·²ç»æ›´æ–°ï¼ˆæ•°æ®ä¸ä¸€è‡´ï¼‰
        current_app.logger.error(f"æ•°æ®åº“æ“ä½œå¤±è´¥: {e}")
        return jsonify({"error": "åˆ›å»ºå¤±è´¥"}), 500

# åå°ä»»åŠ¡çº¿ç¨‹ï¼ˆå¯èƒ½æ³„æ¼ï¼‰
background_threads = []

def start_background_sync():
    """å¯åŠ¨åå°åŒæ­¥ä»»åŠ¡ï¼ˆçº¿ç¨‹æ³„æ¼ï¼‰"""
    def sync_task():
        while True:
            try:
                # æ‰§è¡ŒåŒæ­¥
                sync_data()
                time.sleep(60)
            except Exception as e:
                current_app.logger.error(f"åŒæ­¥ä»»åŠ¡é”™è¯¯: {e}")
                time.sleep(10)

    # åˆ›å»ºçº¿ç¨‹ä½†ä¸è®°å½•å¼•ç”¨
    thread = threading.Thread(target=sync_task, daemon=True)
    thread.start()

    # é—®é¢˜ï¼šçº¿ç¨‹å¼•ç”¨ä¿å­˜åœ¨å…¨å±€åˆ—è¡¨ï¼Œä»ä¸æ¸…ç†
    background_threads.append(thread)

    return len(background_threads)

def sync_data():
    """åŒæ­¥æ•°æ®ï¼ˆå¯èƒ½äº§ç”Ÿæ­»é”ï¼‰"""
    # æ¨¡æ‹Ÿæ•°æ®åŒæ­¥
    time.sleep(5)
    return True
''')

    # æ™ºèƒ½ä½“ç¬¬ä¸‰ä¸ªæƒ³çœ‹çš„æ–‡ä»¶ï¼šroutes.py
    routes_file = os.path.join(CODE_BASE_PATH, "app/routes.py")
    os.makedirs(os.path.dirname(routes_file), exist_ok=True)

    with open(routes_file, 'w') as f:
        f.write('''"""
åº”ç”¨è·¯ç”±é…ç½®
åŒ…å«è·¯ç”±å®šä¹‰å’Œä¸­é—´ä»¶é…ç½®
æ™ºèƒ½ä½“æœç´¢çš„ç¬¬ä¸‰ä¸ªæ–‡ä»¶
"""

from flask import Flask, request, g, jsonify
import time
from app.controllers.data_controller import DataController
from app.api.v2.data import data_bp
from app.middlewares.auth import auth_middleware
from app.middlewares.logging import request_logger

def create_app():
    """åˆ›å»ºFlaskåº”ç”¨"""
    app = Flask(__name__)

    # é…ç½®
    app.config['JSONIFY_PRETTYPRINT_REGULAR'] = True
    app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB

    # æ³¨å†Œä¸­é—´ä»¶
    @app.before_request
    def before_request():
        """è¯·æ±‚å‰ä¸­é—´ä»¶"""
        g.start_time = time.time()

        # é—®é¢˜ï¼šæ¯ä¸ªè¯·æ±‚éƒ½åˆ›å»ºæ•°æ®åº“è¿æ¥
        from app.utils.db_manager import DatabaseManager
        g.db = DatabaseManager()

        # é—®é¢˜ï¼šæ¯ä¸ªè¯·æ±‚éƒ½åˆ›å»ºRedisè¿æ¥
        from app.utils.redis_client import RedisClient
        g.redis = RedisClient()

        # è®°å½•è¯·æ±‚
        request_logger.log_request(request)

    @app.after_request
    def after_request(response):
        """è¯·æ±‚åä¸­é—´ä»¶"""
        # è®¡ç®—è¯·æ±‚è€—æ—¶
        if hasattr(g, 'start_time'):
            elapsed = time.time() - g.start_time
            response.headers['X-Response-Time'] = f'{elapsed:.3f}s'

            # æ…¢è¯·æ±‚æ—¥å¿—
            if elapsed > 2.0:
                app.logger.warning(f"æ…¢è¯·æ±‚: {request.path} - {elapsed:.3f}s")

        # é—®é¢˜ï¼šè¿æ¥æ²¡æœ‰å…³é—­
        # if hasattr(g, 'db'):
        #     g.db.close()  # ç¼ºå°‘è¿™è¡Œä»£ç 

        # if hasattr(g, 'redis'):
        #     g.redis.close()  # ç¼ºå°‘è¿™è¡Œä»£ç 

        return response

    @app.errorhandler(502)
    def handle_502(error):
        """502é”™è¯¯å¤„ç†"""
        app.logger.error(f"502é”™è¯¯: {request.url} - {str(error)}")

        # é—®é¢˜ï¼šæ²¡æœ‰é™çº§ç­–ç•¥ï¼Œç›´æ¥è¿”å›é”™è¯¯
        return jsonify({
            "error": "Bad Gateway",
            "message": "æ— æ³•è¿æ¥ä¸‹æ¸¸æœåŠ¡",
            "path": request.path,
            "timestamp": time.time()
        }), 502

    @app.errorhandler(504)
    def handle_504(error):
        """504é”™è¯¯å¤„ç†"""
        app.logger.error(f"504é”™è¯¯: {request.url} - {str(error)}")

        # é—®é¢˜ï¼šç½‘å…³è¶…æ—¶æ²¡æœ‰é‡è¯•æœºåˆ¶
        return jsonify({
            "error": "Gateway Timeout",
            "message": "è¯·æ±‚å¤„ç†è¶…æ—¶",
            "suggestion": "è¯·ç¨åé‡è¯•"
        }), 504

    # æ³¨å†Œè“å›¾
    app.register_blueprint(data_bp)

    # æ³¨å†Œæ§åˆ¶å™¨è·¯ç”±
    data_controller = DataController()

    @app.route('/api/v1/data', methods=['GET'])
    def get_data_v1():
        """V1æ•°æ®æ¥å£ï¼ˆå…¼å®¹æ—§ç‰ˆï¼‰"""
        # é—®é¢˜ï¼šV1æ¥å£ä»ç„¶åœ¨ä½¿ç”¨ï¼Œä½†å¯èƒ½æœ‰é—®é¢˜
        return data_controller.get_data_v2()  # å®é™…ä¸Šè°ƒç”¨V2é€»è¾‘

    @app.route('/api/v1/data/<int:data_id>', methods=['PUT'])
    def update_data_v1(data_id):
        """æ›´æ–°æ•°æ®ï¼ˆV1ï¼‰"""
        # é—®é¢˜ï¼šæ²¡æœ‰ç‰ˆæœ¬æ§åˆ¶ï¼Œç›´æ¥è°ƒç”¨æ§åˆ¶å™¨
        return data_controller.update_data(data_id)

    @app.route('/health', methods=['GET'])
    def health_check():
        """å¥åº·æ£€æŸ¥"""
        # é—®é¢˜ï¼šå¥åº·æ£€æŸ¥ä¹Ÿåˆ›å»ºæ•°æ®åº“è¿æ¥
        try:
            from app.utils.db_manager import DatabaseManager
            db = DatabaseManager()
            db.execute("SELECT 1")

            from app.utils.redis_client import RedisClient
            redis = RedisClient()
            redis.ping()

            return jsonify({"status": "healthy"})
        except Exception as e:
            app.logger.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return jsonify({"status": "unhealthy", "error": str(e)}), 500

    @app.route('/metrics', methods=['GET'])
    def metrics():
        """åº”ç”¨æŒ‡æ ‡"""
        import psutil
        import threading

        metrics_data = {
            "memory": psutil.virtual_memory().percent,
            "cpu": psutil.cpu_percent(interval=1),
            "threads": threading.active_count(),
            "uptime": time.time() - app.start_time if hasattr(app, 'start_time') else 0
        }

        # é—®é¢˜ï¼šæ¯æ¬¡è°ƒç”¨éƒ½æ”¶é›†å¤§é‡æŒ‡æ ‡
        # æ”¶é›†æ‰€æœ‰çº¿ç¨‹ä¿¡æ¯ï¼ˆå¯èƒ½æ³„æ¼æ•æ„Ÿä¿¡æ¯ï¼‰
        threads_info = []
        for thread in threading.enumerate():
            threads_info.append({
                "name": thread.name,
                "daemon": thread.daemon,
                "alive": thread.is_alive()
            })

        metrics_data["threads_detail"] = threads_info

        return jsonify(metrics_data)

    # è®°å½•åº”ç”¨å¯åŠ¨æ—¶é—´
    app.start_time = time.time()

    return app

# å…¨å±€åº”ç”¨å®ä¾‹
app = create_app()

if __name__ == '__main__':
    # é—®é¢˜ï¼šå¼€å‘æœåŠ¡å™¨é…ç½®ä¸å½“
    app.run(
        host='0.0.0.0',  # å…è®¸æ‰€æœ‰IPè®¿é—®
        port=5000,
        debug=True,  # ç”Ÿäº§ç¯å¢ƒä¸åº”å¼€å¯debug
        threaded=True  # å¤šçº¿ç¨‹æ¨¡å¼ï¼Œä½†å¯èƒ½äº§ç”Ÿçº¿ç¨‹å®‰å…¨é—®é¢˜
    )
''')

    # åˆ›å»ºæœåŠ¡å±‚æ–‡ä»¶ï¼ˆæ™ºèƒ½ä½“æåˆ°çš„ app/services/ï¼‰
    service_file = os.path.join(CODE_BASE_PATH, "app/services/data_service.py")
    os.makedirs(os.path.dirname(service_file), exist_ok=True)

    with open(service_file, 'w') as f:
        f.write('''"""
æ•°æ®æœåŠ¡å±‚
åŒ…å«ä¸šåŠ¡é€»è¾‘å’Œæ•°æ®åº“æ“ä½œ
æ™ºèƒ½ä½“æœç´¢çš„æœåŠ¡å±‚ä»£ç 
"""

import time
import json
from typing import List, Dict, Any
from threading import Lock

class DataService:
    def __init__(self):
        self.cache = {}  # æœ¬åœ°ç¼“å­˜ï¼ˆå¯èƒ½å†…å­˜æ³„æ¼ï¼‰
        self.locks = {}  # ç»†ç²’åº¦é”å­—å…¸
        self.connection_pool = []  # æ¨¡æ‹Ÿè¿æ¥æ± 

        # é—®é¢˜ï¼šå…¨å±€ç»Ÿè®¡åˆ—è¡¨
        self.request_stats = []  # ä»ä¸æ¸…ç†çš„ç»Ÿè®¡æ•°æ®

    def get_large_dataset(self, filters: Dict[str, Any]) -> List[Dict]:
        """
        è·å–å¤§æ•°æ®é›†ï¼ˆæ€§èƒ½é—®é¢˜ï¼‰
        """
        # é—®é¢˜1: å…¨è¡¨æ‰«æ
        sql = "SELECT * FROM large_data_table"

        if filters:
            # åŠ¨æ€æ‹¼æ¥SQLï¼ˆSQLæ³¨å…¥é£é™©ï¼‰
            conditions = []
            for key, value in filters.items():
                conditions.append(f"{key} = '{value}'")  # å­—ç¬¦ä¸²æ‹¼æ¥ï¼Œå±é™©ï¼

            if conditions:
                sql += " WHERE " + " AND ".join(conditions)

        # é—®é¢˜2: æ²¡æœ‰åˆ†é¡µ
        sql += " ORDER BY created_at DESC"
        # ç¼ºå°‘ LIMIT å­å¥

        # æ¨¡æ‹Ÿæ‰§è¡Œæ…¢æŸ¥è¯¢
        time.sleep(3.0)  # è¶…è¿‡2ç§’çš„æ…¢æŸ¥è¯¢é˜ˆå€¼

        # è¿”å›å¤§é‡æ•°æ®
        result = []
        for i in range(10000):  # æ¨¡æ‹Ÿå¤§æ•°æ®é‡
            result.append({
                "id": i,
                "data": "x" * 1024,  # æ¯ä¸ªè®°å½•1KB
                "timestamp": time.time()
            })

        # é—®é¢˜3: ç¼“å­˜å¤§ç»“æœé›†
        cache_key = f"dataset:{str(filters)}"
        self.cache[cache_key] = result  # ç¼“å­˜å¤§å¯¹è±¡

        return result

    def process_batch_transaction(self, items: List[Dict]) -> bool:
        """
        æ‰¹é‡äº‹åŠ¡å¤„ç†ï¼ˆæ­»é”é£é™©ï¼‰
        """
        from app.utils.db_manager import DatabaseManager

        db = DatabaseManager()

        try:
            # å¼€å§‹äº‹åŠ¡
            db.begin_transaction()

            # é—®é¢˜ï¼šå¾ªç¯ä¸­æ‰§è¡Œæ•°æ®åº“æ“ä½œï¼ˆæ€§èƒ½å·®ï¼‰
            for item in items:
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨
                existing = db.query(f"SELECT id FROM items WHERE id={item['id']} FOR UPDATE")

                if existing:
                    # æ›´æ–°ï¼ˆå¯èƒ½æ­»é”ï¼‰
                    db.execute(f"""
                        UPDATE items 
                        SET value='{item['value']}', updated_at=NOW() 
                        WHERE id={item['id']}
                    """)
                else:
                    # æ’å…¥
                    db.execute(f"""
                        INSERT INTO items (id, value) 
                        VALUES ({item['id']}, '{item['value']}')
                    """)

                # é—®é¢˜ï¼šæ¯æ¬¡æ“ä½œåéƒ½è®°å½•æ—¥å¿—ï¼ˆI/Oå¯†é›†ï¼‰
                self._log_operation(item)

                # é—®é¢˜ï¼šå°å»¶è¿Ÿå¢åŠ æ­»é”æ¦‚ç‡
                time.sleep(0.01)

            # æäº¤äº‹åŠ¡
            db.commit()
            return True

        except Exception as e:
            # å›æ»š
            db.rollback()
            print(f"äº‹åŠ¡å¤±è´¥: {e}")

            # é—®é¢˜ï¼šæ²¡æœ‰é‡è¯•æœºåˆ¶
            return False

    def _log_operation(self, item: Dict):
        """è®°å½•æ“ä½œæ—¥å¿—ï¼ˆå¯èƒ½äº§ç”Ÿå¤§é‡æ—¥å¿—ï¼‰"""
        log_entry = {
            "timestamp": time.time(),
            "operation": "process_item",
            "item_id": item.get("id"),
            "thread": threading.current_thread().name
        }

        # æ·»åŠ åˆ°å…¨å±€åˆ—è¡¨ï¼ˆå†…å­˜æ³„æ¼ï¼‰
        self.request_stats.append(log_entry)

        # å†™å…¥æ–‡ä»¶ï¼ˆåŒæ­¥I/Oï¼Œæ€§èƒ½å·®ï¼‰
        with open("operation_logs.txt", "a") as f:
            f.write(json.dumps(log_entry) + "\n")

    def get_with_cache_through(self, key: str) -> Any:
        """
        ç¼“å­˜ç©¿é€é—®é¢˜ç¤ºä¾‹
        """
        # å…ˆæŸ¥æœ¬åœ°ç¼“å­˜
        if key in self.cache:
            return self.cache[key]

        # æŸ¥Redis
        from app.utils.redis_client import RedisClient
        redis = RedisClient()

        cached = redis.get(key)
        if cached:
            # æ›´æ–°æœ¬åœ°ç¼“å­˜
            self.cache[key] = json.loads(cached)
            return self.cache[key]

        # æŸ¥æ•°æ®åº“ï¼ˆç¼“å­˜ç©¿é€ï¼‰
        from app.utils.db_manager import DatabaseManager
        db = DatabaseManager()

        result = db.query(f"SELECT * FROM cache_data WHERE cache_key='{key}'")

        if result:
            # ç¼“å­˜ç»“æœ
            redis.set(key, json.dumps(result), expire=300)
            self.cache[key] = result
            return result
        else:
            # é—®é¢˜ï¼šæŸ¥è¯¢ä¸å­˜åœ¨çš„æ•°æ®ï¼Œæ²¡æœ‰ç©ºå€¼ç¼“å­˜
            return None  # æ¯æ¬¡éƒ½ä¼šæŸ¥è¯¢æ•°æ®åº“

    def cleanup_old_data(self):
        """æ¸…ç†æ—§æ•°æ®ï¼ˆå¯èƒ½é•¿æ—¶é—´é”è¡¨ï¼‰"""
        from app.utils.db_manager import DatabaseManager

        db = DatabaseManager()

        # é—®é¢˜ï¼šDELETE without WHERE (å±é™©ï¼Œä½†è¿™é‡Œæ˜¯æœ‰WHERE)
        # é—®é¢˜ï¼šå¤§äº‹åŠ¡ï¼Œå¯èƒ½é”è¡¨å¾ˆä¹…
        db.execute("""
            DELETE FROM old_data 
            WHERE created_at < NOW() - INTERVAL 90 DAY
        """)

        # é—®é¢˜ï¼šæ²¡æœ‰LIMITï¼Œå¯èƒ½åˆ é™¤å¤§é‡æ•°æ®
        # é—®é¢˜ï¼šæ²¡æœ‰åˆ†æ‰¹æ¬¡åˆ é™¤

        time.sleep(10)  # æ¨¡æ‹Ÿé•¿æ—¶é—´æ“ä½œ

        return True

# å…¨å±€æœåŠ¡å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼Œä½†å¯èƒ½çº¿ç¨‹ä¸å®‰å…¨ï¼‰
_data_service_instance = None

def get_data_service():
    """è·å–æ•°æ®æœåŠ¡å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
    global _data_service_instance

    if _data_service_instance is None:
        # é—®é¢˜ï¼šæ²¡æœ‰é”ï¼Œå¤šçº¿ç¨‹å¯èƒ½åˆ›å»ºå¤šä¸ªå®ä¾‹
        _data_service_instance = DataService()

    return _data_service_instance
''')

    # åˆ›å»ºå·¥å…·ç±»æ–‡ä»¶
    utils_dir = os.path.join(CODE_BASE_PATH, "app/utils")
    os.makedirs(utils_dir, exist_ok=True)

    # Rediså®¢æˆ·ç«¯
    redis_file = os.path.join(utils_dir, "redis_client.py")
    with open(redis_file, 'w') as f:
        f.write('''"""
Rediså®¢æˆ·ç«¯å·¥å…·
åŒ…å«è¿æ¥æ³„æ¼å’Œé…ç½®é—®é¢˜
"""

import redis
import time
from typing import Optional, Any
import threading

class RedisClient:
    def __init__(self, host='localhost', port=6379, db=0):
        self.host = host
        self.port = port
        self.db = db

        # é—®é¢˜ï¼šæ²¡æœ‰ä½¿ç”¨è¿æ¥æ± 
        self.connection = None

        # é—®é¢˜ï¼šç»Ÿè®¡ä¿¡æ¯åˆ—è¡¨ä»ä¸æ¸…ç†
        self.connection_stats = []

    def get_connection(self):
        """è·å–è¿æ¥ï¼ˆè¿æ¥æ³„æ¼ï¼‰"""
        if self.connection is None or not self.connection.ping():
            # æ¯æ¬¡åˆ›å»ºæ–°è¿æ¥
            self.connection = redis.Redis(
                host=self.host,
                port=self.port,
                db=self.db,
                socket_connect_timeout=5,
                socket_timeout=None  # é—®é¢˜ï¼šæ²¡æœ‰è®¾ç½®socketè¶…æ—¶
            )

        # è®°å½•è¿æ¥ä¿¡æ¯ï¼ˆå†…å­˜æ³„æ¼ï¼‰
        self.connection_stats.append({
            "timestamp": time.time(),
            "action": "get_connection"
        })

        return self.connection

    def get(self, key: str) -> Optional[str]:
        """è·å–å€¼"""
        conn = self.get_connection()

        try:
            # é—®é¢˜ï¼šæ²¡æœ‰é‡è¯•æœºåˆ¶
            value = conn.get(key)
            return value.decode('utf-8') if value else None
        except redis.exceptions.ConnectionError as e:
            print(f"Redisè¿æ¥é”™è¯¯: {e}")
            return None
        except Exception as e:
            print(f"Redisæ“ä½œé”™è¯¯: {e}")
            return None

    def set(self, key: str, value: Any, expire: Optional[int] = None):
        """è®¾ç½®å€¼"""
        conn = self.get_connection()

        try:
            if expire:
                conn.setex(key, expire, value)
            else:
                conn.set(key, value)  # æ°¸ä¹…ç¼“å­˜

            # é—®é¢˜ï¼šæ²¡æœ‰è¿”å›å€¼éªŒè¯
            return True
        except Exception as e:
            print(f"Redisè®¾ç½®å¤±è´¥: {e}")
            return False

    def delete(self, key: str):
        """åˆ é™¤é”®"""
        conn = self.get_connection()

        try:
            conn.delete(key)
            return True
        except Exception as e:
            print(f"Redisåˆ é™¤å¤±è´¥: {e}")
            return False

    def pipeline(self):
        """æµæ°´çº¿æ“ä½œï¼ˆå¯èƒ½é˜»å¡ï¼‰"""
        conn = self.get_connection()
        return conn.pipeline()

    def close(self):
        """å…³é—­è¿æ¥ï¼ˆé€šå¸¸ä¸è¢«è°ƒç”¨ï¼‰"""
        if self.connection:
            self.connection.close()
            self.connection = None

    def __del__(self):
        """ææ„å‡½æ•°ï¼ˆä¸ä¸€å®šè¢«è°ƒç”¨ï¼‰"""
        try:
            self.close()
        except:
            pass

# å…¨å±€Redisè¿æ¥ï¼ˆå¯èƒ½æ³„æ¼ï¼‰
_global_redis = None

def get_redis():
    """è·å–å…¨å±€Rediså®ä¾‹"""
    global _global_redis

    if _global_redis is None:
        _global_redis = RedisClient()

    return _global_redis

# è¿æ¥æ³„æ¼ç¤ºä¾‹
class ConnectionLeakExample:
    def __init__(self):
        self.connections = []

    def leak_connections(self):
        """æ•…æ„æ³„æ¼è¿æ¥"""
        for i in range(100):
            conn = RedisClient()
            self.connections.append(conn)  # ä¿å­˜å¼•ç”¨ï¼Œé˜»æ­¢åƒåœ¾å›æ”¶
            # ä¸è°ƒç”¨ conn.close()
''')

    # æ•°æ®åº“ç®¡ç†å™¨
    db_file = os.path.join(utils_dir, "db_manager.py")
    with open(db_file, 'w') as f:
        f.write('''"""
æ•°æ®åº“ç®¡ç†å™¨
åŒ…å«è¿æ¥ç®¡ç†å’ŒæŸ¥è¯¢é—®é¢˜
"""

import time
import threading
from typing import List, Dict, Any, Optional

class DatabaseManager:
    def __init__(self, max_connections=100):
        self.max_connections = max_connections
        self.connections = []  # è¿æ¥æ± 
        self.active_connections = 0

        # é—®é¢˜ï¼šå…¨å±€é”ï¼Œå¯èƒ½æˆä¸ºç“¶é¢ˆ
        self.lock = threading.Lock()

        # é—®é¢˜ï¼šæŸ¥è¯¢ç¼“å­˜ï¼ˆå¯èƒ½å†…å­˜æ³„æ¼ï¼‰
        self.query_cache = {}

    def get_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        with self.lock:
            if len(self.connections) > 0:
                # å¤ç”¨è¿æ¥
                conn = self.connections.pop()
                self.active_connections += 1
                return conn
            elif self.active_connections < self.max_connections:
                # åˆ›å»ºæ–°è¿æ¥
                self.active_connections += 1
                return self._create_connection()
            else:
                # é—®é¢˜ï¼šæ²¡æœ‰ç­‰å¾…æœºåˆ¶ï¼Œç›´æ¥æŠ›å¼‚å¸¸
                raise Exception("æ•°æ®åº“è¿æ¥æ± è€—å°½")

    def _create_connection(self):
        """åˆ›å»ºæ–°è¿æ¥ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        # æ¨¡æ‹Ÿè¿æ¥åˆ›å»ºè€—æ—¶
        time.sleep(0.1)

        return {
            "id": threading.get_ident(),
            "created_at": time.time(),
            "last_used": time.time()
        }

    def release_connection(self, conn):
        """é‡Šæ”¾è¿æ¥ï¼ˆå¯èƒ½ä¸è¢«è°ƒç”¨ï¼‰"""
        with self.lock:
            # é—®é¢˜ï¼šè¿æ¥å¯èƒ½å·²ç»æ— æ•ˆï¼Œä½†æ²¡æœ‰æ£€æŸ¥
            self.connections.append(conn)
            self.active_connections -= 1

    def query(self, sql: str) -> List[Dict]:
        """æ‰§è¡ŒæŸ¥è¯¢"""
        conn = self.get_connection()

        try:
            # é—®é¢˜ï¼šSQLæ³¨å…¥é£é™©ï¼ˆå¦‚æœå¤–éƒ¨ä¼ å…¥ï¼‰
            # é—®é¢˜ï¼šæ²¡æœ‰æŸ¥è¯¢è¶…æ—¶

            # æ£€æŸ¥ç¼“å­˜ï¼ˆå¯èƒ½è¿”å›æ—§æ•°æ®ï¼‰
            cache_key = hash(sql)
            if cache_key in self.query_cache:
                # é—®é¢˜ï¼šç¼“å­˜æ²¡æœ‰è¿‡æœŸæ—¶é—´
                cached = self.query_cache.get(cache_key)
                if time.time() - cached.get("cached_at", 0) < 60:
                    return cached.get("data", [])

            # æ¨¡æ‹ŸæŸ¥è¯¢è€—æ—¶
            time.sleep(1.5)  # æ…¢æŸ¥è¯¢

            # æ¨¡æ‹Ÿç»“æœ
            result = [{"id": i, "value": f"row_{i}"} for i in range(100)]

            # æ›´æ–°ç¼“å­˜
            self.query_cache[cache_key] = {
                "data": result,
                "cached_at": time.time()
            }

            return result

        except Exception as e:
            print(f"æŸ¥è¯¢å¤±è´¥: {e}")
            return []
        finally:
            # é—®é¢˜ï¼šè¿™é‡Œåº”è¯¥è°ƒç”¨ release_connectionï¼Œä½†å¯èƒ½å¿˜è®°
            # self.release_connection(conn)
            pass

    def execute(self, sql: str) -> bool:
        """æ‰§è¡Œæ›´æ–°"""
        conn = self.get_connection()

        try:
            # é—®é¢˜ï¼šæ²¡æœ‰äº‹åŠ¡ç®¡ç†
            # é—®é¢˜ï¼šæ²¡æœ‰é‡è¯•æœºåˆ¶

            # æ¨¡æ‹Ÿæ‰§è¡Œè€—æ—¶
            time.sleep(0.5)

            return True
        except Exception as e:
            print(f"æ‰§è¡Œå¤±è´¥: {e}")
            return False
        finally:
            # åŒæ ·çš„é—®é¢˜ï¼šè¿æ¥å¯èƒ½æ²¡æœ‰é‡Šæ”¾
            pass

    def begin_transaction(self):
        """å¼€å§‹äº‹åŠ¡"""
        # é—®é¢˜ï¼šæ²¡æœ‰å®ç°åµŒå¥—äº‹åŠ¡
        pass

    def commit(self):
        """æäº¤äº‹åŠ¡"""
        pass

    def rollback(self):
        """å›æ»šäº‹åŠ¡"""
        pass

# å…¨å±€æ•°æ®åº“è¿æ¥ï¼ˆå•ä¾‹ï¼‰
_global_db = None

def get_db():
    """è·å–å…¨å±€æ•°æ®åº“å®ä¾‹"""
    global _global_db

    if _global_db is None:
        _global_db = DatabaseManager()

    return _global_db

# æ­»é”ç¤ºä¾‹
class DeadlockExample:
    def __init__(self):
        self.lock_a = threading.Lock()
        self.lock_b = threading.Lock()

    def method1(self):
        """å¯èƒ½äº§ç”Ÿæ­»é”çš„æ–¹æ³•1"""
        with self.lock_a:
            time.sleep(0.1)
            with self.lock_b:
                return "method1 done"

    def method2(self):
        """å¯èƒ½äº§ç”Ÿæ­»é”çš„æ–¹æ³•2ï¼ˆé”é¡ºåºç›¸åï¼‰"""
        with self.lock_b:
            time.sleep(0.1)
            with self.lock_a:
                return "method2 done"
''')

    # åˆ›å»ºä¸­é—´ä»¶ç›®å½•å’Œæ–‡ä»¶ï¼ˆæ™ºèƒ½ä½“å¯èƒ½æœç´¢çš„ï¼‰
    middleware_dir = os.path.join(CODE_BASE_PATH, "app/middlewares")
    os.makedirs(middleware_dir, exist_ok=True)

    # é”™è¯¯å¤„ç†å™¨
    error_file = os.path.join(middleware_dir, "error_handler.py")
    with open(error_file, 'w') as f:
        f.write('''"""
é”™è¯¯å¤„ç†ä¸­é—´ä»¶
å¤„ç†åº”ç”¨ä¸­çš„å„ç§é”™è¯¯
"""

import time
import traceback
from flask import jsonify, request, g
import logging

class ErrorHandler:
    def __init__(self, app=None):
        self.app = app
        self.error_counts = {}  # é”™è¯¯è®¡æ•°

    def init_app(self, app):
        """åˆå§‹åŒ–åº”ç”¨"""
        self.app = app

        # æ³¨å†Œé”™è¯¯å¤„ç†å™¨
        @app.errorhandler(400)
        def handle_bad_request(error):
            return self._format_error(400, "Bad Request", error)

        @app.errorhandler(404)
        def handle_not_found(error):
            return self._format_error(404, "Not Found", error)

        @app.errorhandler(500)
        def handle_internal_error(error):
            # è®°å½•é”™è¯¯è¯¦æƒ…
            self._log_error(500, error)
            return self._format_error(500, "Internal Server Error", error)

        @app.errorhandler(502)
        def handle_bad_gateway(error):
            # 502é”™è¯¯ç‰¹æ®Šå¤„ç†
            return self._handle_502(error)

    def _format_error(self, code: int, message: str, error: Exception):
        """æ ¼å¼åŒ–é”™è¯¯å“åº”"""
        error_info = {
            "error": {
                "code": code,
                "message": message,
                "path": request.path,
                "timestamp": time.time(),
                "request_id": getattr(g, 'request_id', None)
            }
        }

        # é—®é¢˜ï¼šç”Ÿäº§ç¯å¢ƒä¸åº”è¿”å›å †æ ˆè·Ÿè¸ª
        if self.app and self.app.debug:
            error_info["error"]["traceback"] = traceback.format_exc()

        # å¢åŠ é”™è¯¯è®¡æ•°
        self._increment_error_count(code)

        return jsonify(error_info), code

    def _handle_502(self, error: Exception):
        """å¤„ç†502é”™è¯¯"""
        # é—®é¢˜ï¼šæ²¡æœ‰é™çº§ç­–ç•¥
        error_info = {
            "error": {
                "code": 502,
                "message": "Bad Gateway",
                "description": "æ— æ³•è¿æ¥åˆ°ä¸‹æ¸¸æœåŠ¡",
                "suggestion": "è¯·ç¨åé‡è¯•",
                "timestamp": time.time()
            }
        }

        # è®°å½•502é”™è¯¯
        logging.error(f"502é”™è¯¯: {request.url} - {str(error)}")

        # é—®é¢˜ï¼šé”™è¯¯è®¡æ•°å¯èƒ½æ— é™å¢é•¿
        self._increment_error_count(502)

        return jsonify(error_info), 502

    def _increment_error_count(self, code: int):
        """å¢åŠ é”™è¯¯è®¡æ•°"""
        if code not in self.error_counts:
            self.error_counts[code] = 0

        self.error_counts[code] += 1

        # é—®é¢˜ï¼šä»ä¸æ¸…ç†æ—§è®¡æ•°
        # åº”è¯¥å®šæœŸæ¸…ç†æˆ–è®¾ç½®ä¸Šé™

    def _log_error(self, code: int, error: Exception):
        """è®°å½•é”™è¯¯"""
        logging.error(f"é”™è¯¯ {code}: {str(error)}")

        # é—®é¢˜ï¼šåŒæ­¥å†™å…¥æ—¥å¿—æ–‡ä»¶ï¼ˆæ€§èƒ½å·®ï¼‰
        with open("error_logs.txt", "a") as f:
            f.write(f"{time.time()}: {code} - {str(error)}\\n")

    def get_error_stats(self):
        """è·å–é”™è¯¯ç»Ÿè®¡"""
        return self.error_counts

# å…¨å±€é”™è¯¯å¤„ç†å™¨
error_handler = ErrorHandler()
''')

    # æ—¥å¿—ä¸­é—´ä»¶
    logging_file = os.path.join(middleware_dir, "logging.py")
    with open(logging_file, 'w') as f:
        f.write('''"""
æ—¥å¿—ä¸­é—´ä»¶
è®°å½•è¯·æ±‚å’Œå“åº”æ—¥å¿—
"""

import time
import json
from flask import request, g
import logging

class RequestLogger:
    def __init__(self):
        self.log_queue = []  # å†…å­˜ä¸­çš„æ—¥å¿—é˜Ÿåˆ—

    def log_request(self, request):
        """è®°å½•è¯·æ±‚"""
        start_time = time.time()

        request_info = {
            "method": request.method,
            "path": request.path,
            "query_string": request.query_string.decode('utf-8') if request.query_string else "",
            "client_ip": request.remote_addr,
            "user_agent": request.user_agent.string,
            "start_time": start_time,
            "request_id": id(request)  # ç®€å•è¯·æ±‚ID
        }

        # ä¿å­˜åˆ°gå¯¹è±¡
        g.request_info = request_info
        g.start_time = start_time

        # é—®é¢˜ï¼šåŒæ­¥è®°å½•æ—¥å¿—ï¼ˆå½±å“æ€§èƒ½ï¼‰
        logging.info(f"è¯·æ±‚å¼€å§‹: {request.method} {request.path}")

        # æ·»åŠ åˆ°å†…å­˜é˜Ÿåˆ—ï¼ˆå¯èƒ½å†…å­˜æ³„æ¼ï¼‰
        self.log_queue.append(request_info)

        # æ¸…ç†æ—§æ—¥å¿—ï¼ˆä½†å¯èƒ½ä¸æ‰§è¡Œï¼‰
        if len(self.log_queue) > 1000:
            self.log_queue = self.log_queue[-500:]  # åªä¿ç•™æœ€è¿‘500æ¡

    def log_response(self, response):
        """è®°å½•å“åº”"""
        if hasattr(g, 'start_time'):
            elapsed = time.time() - g.start_time

            response_info = {
                "status_code": response.status_code,
                "elapsed": elapsed,
                "end_time": time.time()
            }

            # æ…¢è¯·æ±‚è­¦å‘Š
            if elapsed > 2.0:
                logging.warning(f"æ…¢è¯·æ±‚: {request.path} - {elapsed:.3f}s")

            # æ·»åŠ åˆ°å†…å­˜é˜Ÿåˆ—
            if hasattr(g, 'request_info'):
                log_entry = {**g.request_info, **response_info}
                self.log_queue.append(log_entry)

            # åŒæ­¥å†™å…¥æ–‡ä»¶ï¼ˆæ€§èƒ½å·®ï¼‰
            self._write_to_file(response_info)

    def _write_to_file(self, response_info):
        """å†™å…¥æ–‡ä»¶"""
        try:
            with open("request_logs.txt", "a") as f:
                f.write(json.dumps(response_info) + "\\n")
        except Exception as e:
            logging.error(f"å†™å…¥æ—¥å¿—å¤±è´¥: {e}")

    def get_logs(self, limit=100):
        """è·å–æ—¥å¿—"""
        return self.log_queue[-limit:]

# å…¨å±€æ—¥å¿—è®°å½•å™¨
request_logger = RequestLogger()
''')

    print("âœ… å·²æ ¹æ®æ™ºèƒ½ä½“æ€è·¯åˆ›å»ºæ¨¡æ‹Ÿæ–‡ä»¶ï¼š")
    print(f"   1. {controller_file}")
    print(f"   2. {api_v2_file}")
    print(f"   3. {routes_file}")
    print(f"   4. {service_file}")
    print(f"   5. {redis_file}")
    print(f"   6. {db_file}")
    print(f"   7. {error_file}")
    print(f"   8. {logging_file}")
    print("\nğŸ“ æ–‡ä»¶ç»“æ„ï¼š")
    print("   mock_codebase/")
    print("   â”œâ”€â”€ app/")
    print("   â”‚   â”œâ”€â”€ controllers/")
    print("   â”‚   â”‚   â””â”€â”€ data_controller.py")
    print("   â”‚   â”œâ”€â”€ api/")
    print("   â”‚   â”‚   â””â”€â”€ v2/")
    print("   â”‚   â”‚       â””â”€â”€ data.py")
    print("   â”‚   â”œâ”€â”€ services/")
    print("   â”‚   â”‚   â””â”€â”€ data_service.py")
    print("   â”‚   â”œâ”€â”€ utils/")
    print("   â”‚   â”‚   â”œâ”€â”€ redis_client.py")
    print("   â”‚   â”‚   â””â”€â”€ db_manager.py")
    print("   â”‚   â”œâ”€â”€ middlewares/")
    print("   â”‚   â”‚   â”œâ”€â”€ error_handler.py")
    print("   â”‚   â”‚   â””â”€â”€ logging.py")
    print("   â”‚   â””â”€â”€ routes.py")


if __name__ == "__main__":
    # åˆ›å»ºæ¨¡æ‹Ÿä»£ç åº“
    create_agent_focused_files()

    print("\nğŸ¯ åˆ›å»ºå®Œæˆï¼ç°åœ¨æ™ºèƒ½ä½“å¯ä»¥ï¼š")
    print("   1. æœç´¢ 'data_controller' â†’ æ‰¾åˆ° app/controllers/data_controller.py")
    print("   2. æœç´¢ 'api/v2/data' â†’ æ‰¾åˆ° app/api/v2/data.py")
    print("   3. æœç´¢ 'routes' â†’ æ‰¾åˆ° app/routes.py")
    print("   4. æœç´¢ 'data_service' â†’ æ‰¾åˆ° app/services/data_service.py")
    print("\nğŸ” æ¯ä¸ªæ–‡ä»¶éƒ½åŒ…å«äº†æ™ºèƒ½ä½“å¯èƒ½å‘ç°çš„é—®é¢˜ï¼š")
    print("   - 502é”™è¯¯ï¼ˆä¸‹æ¸¸æœåŠ¡è°ƒç”¨æ— è¶…æ—¶ï¼‰")
    print("   - æ…¢æŸ¥è¯¢ï¼ˆSQLæ€§èƒ½é—®é¢˜ï¼‰")
    print("   - æ­»é”ï¼ˆå¹¶å‘æ§åˆ¶é—®é¢˜ï¼‰")
    print("   - å†…å­˜æ³„æ¼ï¼ˆå…¨å±€å˜é‡ã€è¿æ¥æœªé‡Šæ”¾ï¼‰")
    print("   - Redisç¼“å­˜é—®é¢˜ï¼ˆç©¿é€ã€å‡»ç©¿ã€é›ªå´©ï¼‰")